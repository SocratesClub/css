{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "# Convolutional Networks\n",
    "---\n",
    "---\n",
    "\n",
    "![image.png](img/chengjun.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl23.png)\n",
    "\n",
    "https://www.deeplearningbook.org/contents/convnets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Convolutional networks (LeCun, 1989), also known as convolutional neural networks, or CNNs, are a specialized kind of neural network for processing data that has a known *grid-like topology*. \n",
    "\n",
    "- time-series data,  1-D grid \n",
    "- image data, 2-D grid of pixels. \n",
    "\n",
    "The name “convolutional neural network” indicates that the network employs a mathematical operation called `convolution`. \n",
    "\n",
    "- Convolution is a specialized kind of linear operation. \n",
    "- Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "<div><img src=img/dl24.png></div>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div><img src=img/cnn.gif></div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "全连接前馈网络在图像处理中的缺点：图像数据维度太高（一个像素点被视为一个维度），而且由于图像很多信息在高层次才体现，在过低的层次分析导致过拟合以及训练效率过低。\n",
    "- Scale up neural networks to process very large images / video sequences\n",
    "    - Sparse connections\n",
    "    - Parameter sharing\n",
    "- Automatically generalize across spatial translations of inputs\n",
    "- Applicable to any input that is laid out on a grid (1-D, 2-D, 3-D, …)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Replace matrix multiplication in neural nets with convolution\n",
    "- Everything else stays the same\n",
    "    - Maximum likelihood\n",
    "    - Back-propagation\n",
    "    - etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Convolution** is a mathematical way of combining two signals to form a third signal. \n",
    "\n",
    "$$(f * g)(t) \\triangleq\\ \\int_{-\\infty}^\\infty f(\\tau) g(t - \\tau) \\, d\\tau$$\n",
    "\n",
    "\n",
    "<center>    \n",
    "<div><img src=img/Convolution_of_box_signal_with_itself2.gif align = center width = 1000px></div> \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Convolution Operation\n",
    "\n",
    "Suppose we are tracking the location of a spaceship with a laser tensor.\n",
    "\n",
    "$x(\\tau)$ denote the location of the `spaceship` at time $\\tau$. Assume that our laser sensor is somewhat noisy, we would like to average together several measurements. But we would like to give more weight to recent measurements with a weight function $w(t)$, wheare $\\tau$ is the age of a measurement. We can obtain a new function $F(t)$ providing a smoothed estimate of the position of the spaceship.\n",
    "\n",
    "$$ F(t) = x*w(t) = \\int x(\\tau) w(t-\\tau) d \\tau$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we are tracking the location of a spaceship with a laser tensor.\n",
    "\n",
    "\n",
    "<center>\n",
    "<div><img src=img/convgaus.gif width = 800px></div></center>\n",
    "\n",
    "https://mathworld.wolfram.com/Convolution.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Convolution network terminology**\n",
    "- input: function $x$\n",
    "- kernal: function $w$\n",
    "- featuremap: output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In machine learning applications, the input is usually **a multidimensional array** of data, and the kernel is usually a multidimensional array of parameters that are adapted by the learning algorithm. We will refer to these multidimensional arrays as **tensors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Discrete Convolution**\n",
    "\n",
    "We only measure at discreted timestamp\n",
    "\n",
    "$$ s(t) = \\sum_{\\tau = -\\infty}^{\\infty} x(\\tau) w(t-\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we often use convolutions over more than one axis at a time. For example, if we use a two-dimensional image $I$ as our input, we probably also want to use a two-dimensional kernel \n",
    "\n",
    "$$S(i, j) = (I ∗ K)(i, j) =\\sum_m \\sum_n I(m, n)K(i − m, j − n)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Discrete convolution can be viewed as multiplication by a matrix\n",
    "\n",
    "![image.png](img/dl25.png)\n",
    "\n",
    "Matrix (Dot) Product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**2D Convolution**\n",
    "\n",
    "![image.png](img/dl26.png)\n",
    "\n",
    "Discrete convolution can be viewed as multiplication by a matrix\n",
    "本质来说，CNN基于一个假设：即使每个像素只偏移一点点，图片的整体信息仍然不变。从而可以用卷积操作高效降维度。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl27.png)\n",
    "\n",
    "**Padding**\n",
    "\n",
    "每次卷积操作都会使图像缩小，角落和边缘区域的像素点使用的很少，即丢失了图像边缘位置的信息。为了解决这两个问题，在卷积操作之前，先对图像周围进行填充，通常为zero-padding，即填充0。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Three Operations\n",
    "- Convolution: like matrix multiplication\n",
    "    - Take an input, produce an output (hidden layer)\n",
    "- “Deconvolution”: like multiplication by transpose of a matrix\n",
    "    - Used to back-propagate error from output to input\n",
    "    - Reconstruction in autoencoder / RBM\n",
    "- Weight gradient computation\n",
    "    - Used to backpropagate error from output to weights\n",
    "    - Accounts for the parameter sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Convolution leverages three important ideas: \n",
    "- sparse interactions, \n",
    "- parameter sharing,  \n",
    "- equivariant representations.\n",
    "        $$f(g(x)) = g(f(x))$$\n",
    "    \n",
    "    if we move the object in the input, its representation wil move the same amount in the outupt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl29.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl30.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl31.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl32.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example: Edge Detection by Convolution\n",
    "\n",
    "![image.png](img/dl33.png)\n",
    "\n",
    "[卷积的直观理解](https://github.com/computational-class/summer-school/blob/master/class_03_%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical layer of a convolutional network consists of **three stages**\n",
    "\n",
    "- In the ﬁrst stage, the layer performs several **convolutions** in parallel to produce aset of linear activations. \n",
    "- In the second stage, each linear activation is run through a nonlinear activation function, such as the rectiﬁed linear activation function. This stage is sometimes called the **detector** stage. \n",
    "- In the third stage, we use **apooling** function to modify the output of the layer further."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![image.png](img/dl35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pooling 池化\n",
    "\n",
    "**A pooling function** replaces the output of the net at a certain location with a summary statistic of the nearby outputs. \n",
    "\n",
    "For example, the **max pooling** (Zhou and Chellappa, 1988) operation reports the maximum output within a rectangular neighborhood. \n",
    "\n",
    "Other popular pooling functions include \n",
    "- the average of a rectangularneighborhood, \n",
    "- the L2 norm (每个元素的平方的和) of a rectangular neighborhood,  \n",
    "- a weighted average based on the distance from the central pixel.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl36.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl37.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl38.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl39.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The human eye is mostly very low resolution, except for a tiny patch called the fovea. The fovea only observes an area about the size of a thumbnail held at arms length. \n",
    "\n",
    "![image.png](img/dl40.png)\n",
    "\n",
    "Though we feel as if we can see an entire scene in high resolution, this is an illusion created by the subconscious part of our brain, as it stitches together several glimpses of small areas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most convolutional networks actually receive large full-resolution photographs as input. \n",
    "\n",
    "The human brain makes several eye movements called saccadesto glimpse the most visually salient or task-relevant parts of a scene. \n",
    "\n",
    "Incorporating similar attention mechanisms into deep learning models is an active research direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "https://atcold.github.io/pytorch-Deep-Learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T03:03:51.733295Z",
     "start_time": "2020-04-06T03:03:51.729210Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 使用PyTorch建立卷积神经网络并处理MNIST数据\n",
    "\n",
    "https://computational-communication.com/pytorch-mnist/\n",
    "\n",
    "https://github.com/computational-class/summer-school/blob/master/class_04_CNN.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl41.png)\n",
    "\n",
    "https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:33:47.548559Z",
     "start_time": "2020-05-26T01:33:46.684453Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:33:47.626245Z",
     "start_time": "2020-05-26T01:33:47.550534Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='../data/',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "                            # change it to False after you have downloaded the data\n",
    "\n",
    "test_dataset = datasets.MNIST(root='../data/',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:33:49.679584Z",
     "start_time": "2020-05-26T01:33:49.677092Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "batch_size = 100 \n",
    "learning_rate = 0.001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:33:52.232286Z",
     "start_time": "2020-05-26T01:33:52.228728Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:33:54.019913Z",
     "start_time": "2020-05-26T01:33:53.667867Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import torchvision\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T06:48:36.740915Z",
     "start_time": "2020-05-26T06:48:36.737205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:35:33.201987Z",
     "start_time": "2020-05-26T01:35:33.058322Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) tensor(9) tensor(9) tensor(6) tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:5]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:35:33.984996Z",
     "start_time": "2020-05-26T01:35:33.975962Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# CNN Model (2 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # in_channels=1 (图片只有一种颜色), out_channels=16(16个不同的卷积核，产生16个卷积结果)\n",
    "            # kernel_size: 每个卷积核的边长， padding：图片周围加两圈0\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            # apply batch normalization\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "            # 每个kernel take all channels from input and comebine them into one channel\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 10) \n",
    "        # a linear transformation AX +b: 把7*7*32维的张量 reduce 成 (x1,..,x10)的10维向量\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:35:34.715612Z",
     "start_time": "2020-05-26T01:35:34.712231Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T06:26:29.597512Z",
     "start_time": "2020-05-26T06:26:29.590770Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0627, 0.4078, 0.5529, 0.9451, 0.7529,\n",
       "          0.5529, 0.4078, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1255,\n",
       "          0.2235, 0.3725, 0.6627, 0.8471, 0.9882, 0.9882, 0.9882, 0.9922,\n",
       "          0.9882, 0.9882, 0.2078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.8980,\n",
       "          0.9882, 0.9882, 0.9882, 0.9922, 0.9882, 0.8784, 0.6588, 0.6588,\n",
       "          0.8784, 0.9882, 0.8941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4431, 0.9922,\n",
       "          0.9882, 0.9882, 0.7922, 0.5490, 0.2549, 0.0745, 0.0000, 0.0000,\n",
       "          0.4667, 0.9882, 0.9882, 0.3961, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6392,\n",
       "          0.4431, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2745, 0.9922, 0.9922, 0.4471, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n",
       "          0.7333, 0.9882, 0.9882, 0.4431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137,\n",
       "          0.9882, 0.9882, 0.9882, 0.4431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5020,\n",
       "          0.9882, 0.9882, 0.9882, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6039,\n",
       "          0.9922, 0.9922, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1020, 0.9922,\n",
       "          0.9882, 0.9882, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7843, 0.9922,\n",
       "          0.9882, 0.7294, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8824, 0.9922,\n",
       "          0.8902, 0.1686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3843, 0.9922, 1.0000,\n",
       "          0.7216, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0510, 0.8196, 0.9882, 0.9686,\n",
       "          0.2549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 0.9882, 0.9882, 0.7333,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.6157, 0.9882, 0.9882, 0.4431,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.3608, 0.9922, 0.9922, 0.9922, 0.0510,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5529, 0.9882, 0.9882, 0.9882, 0.4431,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5529, 0.9882, 0.9882, 0.6941, 0.0980,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.5529, 0.9882, 0.6431, 0.0588, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T06:24:05.786257Z",
     "start_time": "2020-05-26T06:24:05.782053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:41:34.551970Z",
     "start_time": "2020-05-26T01:35:37.184124Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Iter [200/600] Loss: 0.0932\n",
      "Epoch [1/5], Iter [400/600] Loss: 0.0841\n",
      "Epoch [1/5], Iter [600/600] Loss: 0.0559\n",
      "Epoch [2/5], Iter [200/600] Loss: 0.1323\n",
      "Epoch [2/5], Iter [400/600] Loss: 0.0386\n",
      "Epoch [2/5], Iter [600/600] Loss: 0.0586\n",
      "Epoch [3/5], Iter [200/600] Loss: 0.1227\n",
      "Epoch [3/5], Iter [400/600] Loss: 0.0483\n",
      "Epoch [3/5], Iter [600/600] Loss: 0.0633\n",
      "Epoch [4/5], Iter [200/600] Loss: 0.0392\n",
      "Epoch [4/5], Iter [400/600] Loss: 0.0121\n",
      "Epoch [4/5], Iter [600/600] Loss: 0.0071\n",
      "Epoch [5/5], Iter [200/600] Loss: 0.0134\n",
      "Epoch [5/5], Iter [400/600] Loss: 0.0347\n",
      "Epoch [5/5], Iter [600/600] Loss: 0.0491\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:43:10.143535Z",
     "start_time": "2020-05-26T01:43:07.143909Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/distiller/project/conda/conda-bld/pytorch_1587428061935/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1) # 自动返回数列中最大的value和对应的index\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:43:45.498316Z",
     "start_time": "2020-05-26T01:43:45.360185Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "case=test_dataset[0][0]\n",
    "case=case.reshape(1, 1, 28, 28)\n",
    "plt.imshow(case[0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T01:44:04.173922Z",
     "start_time": "2020-05-26T01:44:04.167163Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cnn(case)\n",
    "pred = output.argmax(dim=1, keepdim=True)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 识别错误的一个例子---即使人工也很难认出这是什么数字，说明CNN的准确率已经达到较高水平\n",
    "imshow(torchvision.utils.make_grid(images[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Chapter 9, Deep Learning. Ian Goodfellow et al. 2016 \n",
    "http://www.deeplearningbook.org/contents/convnets.html\n",
    "- CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization https://poloclub.github.io/cnn-explainer/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/chengjun2.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

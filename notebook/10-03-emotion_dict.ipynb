{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 大连理工大学中文情感词汇\n",
    "\n",
    "### 1. 介绍\n",
    "中文情感词汇本体库是大连理工大学信息检索研究室在林鸿飞教授的指导下经过全体教研室成员的努力整理和标注的一个中文本体资源。该资源从不同角度描述一个中文词汇或者短语，包括词语词性种类、情感类别、情感强度及极性等信息。\n",
    "\n",
    "中文情感词汇本体的情感分类体系是在国外比较有影响的Ekman的6大类情感分类体系的基础上构建的。在Ekman的基础上，词汇本体加入情感类别“好”对褒义情感进行了更细致的划分。最终词汇本体中的情感共分为7大类21小类。\n",
    "构造该资源的宗旨是在情感计算领域，为中文文本情感分析和倾向性分析提供一个便捷可靠的辅助手段。中文情感词汇本体可以用于解决多类别情感分类的问题，同时也可以用于解决一般的倾向性分析的问题。\n",
    "\n",
    "其中，一个情感词可能对应多个情感，情感分类用于刻画情感词的主要情感分类，辅助情感为该情感词在具有主要情感分类的同时含有的其他情感分类。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "http://ir.dlut.edu.cn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ### 2. 情感词汇本体格式\n",
    " \n",
    " | *词语*   | *词性种类* | *词义数* | *词义序号* | *情感分类* | *强度* | *极性* | *辅助情感分类* | *强度* | *极性* |\n",
    "| -------- | ---------- | -------- | ---------- | ---------- | ------ | ------ | -------------- | ------ | ------ |\n",
    "| 无所畏惧 | idiom      | 1        | 1          | PH         | 7      | 1      |                |        |        |\n",
    "| 手头紧   | idiom      | 1        | 1          | NE         | 7      | 0      |                |        |        |\n",
    "| 周到     | adj        | 1        | 1          | PH         | 5      | 1      |                |        |        |\n",
    "| 言过其实 | idiom      | 1        | 1          | NN         | 5      | 2      |                |        |        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.  情感分类及情感强度\n",
    "\n",
    "情感分类按照论文《情感词汇本体的构造》所述，情感分为7大类21小类。\n",
    "情感强度分为1,3,5,7,9五档，9表示强度最大，1为强度最小。\n",
    "\n",
    "\n",
    "\n",
    "| 编号 | 情感大类 | 情感类   | 例词                           |\n",
    "| ---- | -------- | -------- | ------------------------------ |\n",
    "| 1    | 乐       | 快乐(PA) | 喜悦、欢喜、笑眯眯、欢天喜地   |\n",
    "| 2    |          | 安心(PE) | 踏实、宽心、定心丸、问心无愧   |\n",
    "| 3    | 好       | 尊敬(PD) | 恭敬、敬爱、毕恭毕敬、肃然起敬 |\n",
    "| 4    |          | 赞扬(PH) | 英俊、优秀、通情达理、实事求是 |\n",
    "| 5    |          | 相信(PG) | 信任、信赖、可靠、毋庸置疑     |\n",
    "| 6    |          | 喜爱(PB) | 倾慕、宝贝、一见钟情、爱不释手 |\n",
    "| 7    |          | 祝愿(PK) | 渴望、保佑、福寿绵长、万寿无疆 |\n",
    "| 8    | 怒       | 愤怒(NA) | 气愤、恼火、大发雷霆、七窍生烟 |\n",
    "| 9    | 哀       | 悲伤(NB) | 忧伤、悲苦、心如刀割、悲痛欲绝 |\n",
    "| 10   |          | 失望(NJ) | 憾事、绝望、灰心丧气、心灰意冷 |\n",
    "| 11   |          | 疚(NH)   | 内疚、忏悔、过意不去、问心有愧 |\n",
    "| 12   |          | 思(PF)   | 思念、相思、牵肠挂肚、朝思暮想 |\n",
    "| 13   | 惧       | 慌(NI)   | 慌张、心慌、不知所措、手忙脚乱 |\n",
    "| 14   |          | 恐惧(NC) | 胆怯、害怕、担惊受怕、胆颤心惊 |\n",
    "| 15   |          | 羞(NG)   | 害羞、害臊、面红耳赤、无地自容 |\n",
    "| 16   | 恶       | 烦闷(NE) | 憋闷、烦躁、心烦意乱、自寻烦恼 |\n",
    "| 17   |          | 憎恶(ND) | 反感、可耻、恨之入骨、深恶痛绝 |\n",
    "| 18   |          | 贬责(NN) | 呆板、虚荣、杂乱无章、心狠手辣 |\n",
    "| 19   |          | 妒忌(NK) | 眼红、吃醋、醋坛子、嫉贤妒能   |\n",
    "| 20   |          | 怀疑(NL) | 多心、生疑、将信将疑、疑神疑鬼 |\n",
    "| 21   | 惊       | 惊奇(PC) | 奇怪、奇迹、大吃一惊、瞠目结舌 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.  词性种类\n",
    "\t情感词汇本体中的词性种类一共分为7类，分别是名词（noun），动词（verb），形容词（adj），副词（adv），网络词语（nw），成语（idiom），介词短语（prep）。\n",
    "### 5.  极性标注\n",
    "\t每个词在每一类情感下都对应了一个极性。其中，0代表中性，1代表褒义，2代表贬义，3代表兼有褒贬两性。\n",
    "\t注：褒贬标注时，通过词本身和情感共同确定，所以有些情感在一些词中可能极性1，而其他的词中有可能极性为0。\n",
    "### 6.  存储格式及规模\n",
    "\t中文情感本体以excel的格式进行存储，共含有情感词共计27466个，文件大小为1.22M。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T03:25:56.484844Z",
     "start_time": "2021-08-10T03:25:50.827676Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>词语</th>\n",
       "      <th>词性种类</th>\n",
       "      <th>词义数</th>\n",
       "      <th>词义序号</th>\n",
       "      <th>情感分类</th>\n",
       "      <th>强度</th>\n",
       "      <th>极性</th>\n",
       "      <th>辅助情感分类</th>\n",
       "      <th>强度.1</th>\n",
       "      <th>极性.1</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>脏乱</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>糟报</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>早衰</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>责备</td>\n",
       "      <td>verb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>贼眼</td>\n",
       "      <td>noun</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   词语  词性种类  词义数  词义序号 情感分类  强度  极性 辅助情感分类  强度.1  极性.1 Unnamed: 10  \\\n",
       "0  脏乱   adj  1.0   1.0   NN   7   2    NaN   NaN   NaN         NaN   \n",
       "1  糟报   adj  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
       "2  早衰   adj  1.0   1.0   NE   5   2    NaN   NaN   NaN         NaN   \n",
       "3  责备  verb  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
       "4  贼眼  noun  1.0   1.0   NN   5   2    NaN   NaN   NaN         NaN   \n",
       "\n",
       "   Unnamed: 11  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('../data/情感词汇.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T03:26:02.582744Z",
     "start_time": "2021-08-10T03:26:02.578131Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27466, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T03:26:07.732509Z",
     "start_time": "2021-08-10T03:26:07.698257Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>词语</th>\n",
       "      <th>词性种类</th>\n",
       "      <th>词义数</th>\n",
       "      <th>词义序号</th>\n",
       "      <th>情感分类</th>\n",
       "      <th>强度</th>\n",
       "      <th>极性</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>脏乱</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>糟报</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>早衰</td>\n",
       "      <td>adj</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NE</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>责备</td>\n",
       "      <td>verb</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>贼眼</td>\n",
       "      <td>noun</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   词语  词性种类  词义数  词义序号 情感分类  强度  极性\n",
       "0  脏乱   adj  1.0   1.0   NN   7   2\n",
       "1  糟报   adj  1.0   1.0   NN   5   2\n",
       "2  早衰   adj  1.0   1.0   NE   5   2\n",
       "3  责备  verb  1.0   1.0   NN   5   2\n",
       "4  贼眼  noun  1.0   1.0   NN   5   2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['词语', '词性种类', '词义数', '词义序号', '情感分类', '强度', '极性']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T03:26:33.469226Z",
     "start_time": "2021-08-10T03:26:30.069476Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情绪词语列表整理完成\n"
     ]
    }
   ],
   "source": [
    "Happy = []\n",
    "Good = []\n",
    "Surprise = []\n",
    "Anger = []\n",
    "Sad = []\n",
    "Fear = []\n",
    "Disgust = []\n",
    "for idx, row in df.iterrows():\n",
    "    if row['情感分类'] in ['PA', 'PE']:\n",
    "        Happy.append(row['词语'])\n",
    "    if row['情感分类'] in ['PD', 'PH', 'PG', 'PB', 'PK']:\n",
    "        Good.append(row['词语']) \n",
    "    if row['情感分类'] in ['PC']:\n",
    "        Surprise.append(row['词语'])     \n",
    "    if row['情感分类'] in ['NA']:\n",
    "        Anger.append(row['词语'])    \n",
    "    if row['情感分类'] in ['NB', 'NJ', 'NH', 'PF']:\n",
    "        Sad.append(row['词语'])\n",
    "    if row['情感分类'] in ['NI', 'NC', 'NG']:\n",
    "        Fear.append(row['词语'])\n",
    "    if row['情感分类'] in ['NE', 'ND', 'NN', 'NK', 'NL']:\n",
    "        Disgust.append(row['词语'])\n",
    "Positive = Happy + Good +Surprise\n",
    "Negative = Anger + Sad + Fear + Disgust\n",
    "print('情绪词语列表整理完成') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:18:51.413452Z",
     "start_time": "2020-06-12T07:18:51.270733Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length      25\n",
       "positive     0\n",
       "negative     4\n",
       "anger        0\n",
       "disgust      4\n",
       "fear         0\n",
       "sadness      0\n",
       "surprise     0\n",
       "good         0\n",
       "happy        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import time\n",
    "def emotion_caculate(text):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    anger = 0\n",
    "    disgust = 0\n",
    "    fear = 0\n",
    "    sad = 0\n",
    "    surprise = 0\n",
    "    good = 0\n",
    "    happy = 0\n",
    "    wordlist = jieba.lcut(text)\n",
    "    wordset = set(wordlist)\n",
    "    wordfreq = []\n",
    "    for word in wordset:\n",
    "        freq = wordlist.count(word)\n",
    "        if word in Positive:\n",
    "            positive+=freq\n",
    "        if word in Negative:\n",
    "            negative+=freq\n",
    "        if word in Anger:\n",
    "            anger+=freq\n",
    "        if word in Disgust:\n",
    "            disgust+=freq\n",
    "        if word in Fear:\n",
    "            fear+=freq\n",
    "        if word in Sad:\n",
    "            sad+=freq\n",
    "        if word in Surprise:\n",
    "            surprise+=freq\n",
    "        if word in Good:\n",
    "            good+=freq\n",
    "        if word in Happy:\n",
    "            happy+=freq\n",
    "    emotion_info = {\n",
    "        'length':len(wordlist),\n",
    "        'positive': positive,\n",
    "        'negative': negative,\n",
    "        'anger': anger,\n",
    "        'disgust': disgust,\n",
    "        'fear':fear,\n",
    "        'good':good,\n",
    "        'sadness':sad,\n",
    "        'surprise':surprise,\n",
    "        'happy':happy,\n",
    "    }\n",
    "    indexs = ['length', 'positive', 'negative', 'anger', 'disgust','fear','sadness','surprise', 'good', 'happy']\n",
    "    return pd.Series(emotion_info, index=indexs)\n",
    "\n",
    "emotion_caculate(text='这个国家再对这些制造假冒伪劣食品药品的人手软的话，那后果真的会相当糟糕。坐牢？从快判个死刑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T03:27:09.449732Z",
     "start_time": "2021-08-10T03:27:09.287560Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/datalab/bigdata/fu_zhu_2020_jrr_cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9ad491afdfa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#weibo = pd.read_csv('/Users/datalab/bigdata/fu_zhu_2020_jrr_cleaned.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/datalab/bigdata/fu_zhu_2020_jrr_cleaned.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#跳过报错行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchunkSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/datalab/bigdata/fu_zhu_2020_jrr_cleaned.csv'"
     ]
    }
   ],
   "source": [
    "#weibo = pd.read_csv('/Users/datalab/bigdata/fu_zhu_2020_jrr_cleaned.csv')\n",
    "\n",
    "f = open('/Users/datalab/bigdata/fu_zhu_2020_jrr_cleaned.csv',encoding='utf-8')\n",
    "reader = pd.read_table(f,  sep=',',  quotechar='\"', iterator=True, error_bad_lines=False) #跳过报错行\n",
    "chunkSize = 1000\n",
    "weibo_df = reader.get_chunk(chunkSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:19:17.559674Z",
     "start_time": "2020-06-12T07:19:17.533827Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>censorship_type</th>\n",
       "      <th>id_hashed</th>\n",
       "      <th>retweeted_status_hashed</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31T11:58:20Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2e041cb2c8814594aa800d918178a2c3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>突发！武汉发现多名不明原因肺炎患者，没确定是否SARS病毒\\nhttp://t.cn/AiF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-31T13:27:35Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>db0826e6447e40e8346fa0979eb51dfc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#武汉不明原因肺炎不能断定是SARS#望早日出结果 ​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-01T16:59:30Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50e10abb2423ca1fbc89e80fafd48a49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#带着微博去旅行[超话]#缅甸神果 大自然回馈的天然礼物，一小瓶花果油，咳嗽肺病都不是问题！...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-01T17:15:39Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>572454f3f17bb1cb903a45ef5136fc4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【文化视点】还记得这首《大侠霍元甲》的主题曲《万里长城永不倒》吗？12月1日，香港乐坛传来了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-01T21:24:48Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26e4e5727afc852ffe4fc67c91aa125c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#带着微博去旅行[超话]#缅甸神奇花果#探秘大美云南[超话]#微信17869425616\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at censorship_type                         id_hashed  \\\n",
       "0  2019-12-31T11:58:20Z             NaN  2e041cb2c8814594aa800d918178a2c3   \n",
       "1  2019-12-31T13:27:35Z             NaN  db0826e6447e40e8346fa0979eb51dfc   \n",
       "2  2019-12-01T16:59:30Z             NaN  50e10abb2423ca1fbc89e80fafd48a49   \n",
       "3  2019-12-01T17:15:39Z             NaN  572454f3f17bb1cb903a45ef5136fc4c   \n",
       "4  2019-12-01T21:24:48Z             NaN  26e4e5727afc852ffe4fc67c91aa125c   \n",
       "\n",
       "  retweeted_status_hashed                                       text_cleaned  \n",
       "0                     NaN  突发！武汉发现多名不明原因肺炎患者，没确定是否SARS病毒\\nhttp://t.cn/AiF...  \n",
       "1                     NaN                        #武汉不明原因肺炎不能断定是SARS#望早日出结果 ​  \n",
       "2                     NaN  #带着微博去旅行[超话]#缅甸神果 大自然回馈的天然礼物，一小瓶花果油，咳嗽肺病都不是问题！...  \n",
       "3                     NaN  【文化视点】还记得这首《大侠霍元甲》的主题曲《万里长城永不倒》吗？12月1日，香港乐坛传来了...  \n",
       "4                     NaN  #带着微博去旅行[超话]#缅甸神奇花果#探秘大美云南[超话]#微信17869425616\\n...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T05:32:58.513063Z",
     "start_time": "2020-06-12T05:32:58.510743Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# from pandarallel import pandarallel\n",
    "# #并行初始化\n",
    "# pandarallel.initialize()\n",
    "# start = time.time()   \n",
    "# emotion_df = weibo['text_cleaned'].parallel_apply(emotion_caculate)\n",
    "# end = time.time()\n",
    "# print(end-start)\n",
    "# emotion_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:21:49.696649Z",
     "start_time": "2020-06-12T07:19:31.202996Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "emotions = []\n",
    "for i in weibo_df['text_cleaned']:\n",
    "    emotions.append(emotion_caculate(text=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:22:04.254788Z",
     "start_time": "2020-06-12T07:22:04.241289Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length      27\n",
       "positive     1\n",
       "negative     1\n",
       "anger        0\n",
       "disgust      0\n",
       "fear         1\n",
       "sadness      0\n",
       "surprise     0\n",
       "good         1\n",
       "happy        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:22:08.382359Z",
     "start_time": "2020-06-12T07:22:07.988573Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "emotions_df = pd.DataFrame(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:22:09.328965Z",
     "start_time": "2020-06-12T07:22:09.281241Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>censorship_type</th>\n",
       "      <th>id_hashed</th>\n",
       "      <th>retweeted_status_hashed</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>length</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31T11:58:20Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2e041cb2c8814594aa800d918178a2c3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>突发！武汉发现多名不明原因肺炎患者，没确定是否SARS病毒\\nhttp://t.cn/AiF...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-31T13:27:35Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>db0826e6447e40e8346fa0979eb51dfc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#武汉不明原因肺炎不能断定是SARS#望早日出结果 ​</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-01T16:59:30Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50e10abb2423ca1fbc89e80fafd48a49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#带着微博去旅行[超话]#缅甸神果 大自然回馈的天然礼物，一小瓶花果油，咳嗽肺病都不是问题！...</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-01T17:15:39Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>572454f3f17bb1cb903a45ef5136fc4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【文化视点】还记得这首《大侠霍元甲》的主题曲《万里长城永不倒》吗？12月1日，香港乐坛传来了...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-01T21:24:48Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26e4e5727afc852ffe4fc67c91aa125c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#带着微博去旅行[超话]#缅甸神奇花果#探秘大美云南[超话]#微信17869425616\\n...</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at censorship_type                         id_hashed  \\\n",
       "0  2019-12-31T11:58:20Z             NaN  2e041cb2c8814594aa800d918178a2c3   \n",
       "1  2019-12-31T13:27:35Z             NaN  db0826e6447e40e8346fa0979eb51dfc   \n",
       "2  2019-12-01T16:59:30Z             NaN  50e10abb2423ca1fbc89e80fafd48a49   \n",
       "3  2019-12-01T17:15:39Z             NaN  572454f3f17bb1cb903a45ef5136fc4c   \n",
       "4  2019-12-01T21:24:48Z             NaN  26e4e5727afc852ffe4fc67c91aa125c   \n",
       "\n",
       "  retweeted_status_hashed                                       text_cleaned  \\\n",
       "0                     NaN  突发！武汉发现多名不明原因肺炎患者，没确定是否SARS病毒\\nhttp://t.cn/AiF...   \n",
       "1                     NaN                        #武汉不明原因肺炎不能断定是SARS#望早日出结果 ​   \n",
       "2                     NaN  #带着微博去旅行[超话]#缅甸神果 大自然回馈的天然礼物，一小瓶花果油，咳嗽肺病都不是问题！...   \n",
       "3                     NaN  【文化视点】还记得这首《大侠霍元甲》的主题曲《万里长城永不倒》吗？12月1日，香港乐坛传来了...   \n",
       "4                     NaN  #带着微博去旅行[超话]#缅甸神奇花果#探秘大美云南[超话]#微信17869425616\\n...   \n",
       "\n",
       "   length  positive  negative  anger  disgust  fear  sadness  surprise  good  \\\n",
       "0      27         1         1      0        0     1        0         0     1   \n",
       "1      16         0         0      0        0     0        0         0     0   \n",
       "2     107         2         1      0        1     0        0         0     2   \n",
       "3      82         1         2      0        0     0        2         0     1   \n",
       "4      89         2         1      0        1     0        0         1     1   \n",
       "\n",
       "   happy  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo_df = pd.concat([weibo_df, emotions_df], axis=1)\n",
    "weibo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:22:33.114828Z",
     "start_time": "2020-06-12T07:22:33.085978Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>fear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>【#武汉已发现27例肺炎病例#】据武汉卫健委网站，近期部分医疗机构发现接诊的多例肺炎病例与华...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>【#武汉已发现27例肺炎病例#】据武汉卫健委网站，近期部分医疗机构发现接诊的多例肺炎病例与华...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>【武汉卫健委：#已发现27例肺炎病例# 初步分析为病毒性肺炎，未出现人传人现象】近期部分医疗...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>【#武汉病毒性肺炎病例多为海鲜城经营户#】据湖北省相关部门消息，12月以来，武汉市持续开展流...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>【#武汉市卫健委通报肺炎疫情#：27例病例 7例病情严重】#武汉肺炎无明显人传人现象# 近期...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_cleaned  fear\n",
       "906  【#武汉已发现27例肺炎病例#】据武汉卫健委网站，近期部分医疗机构发现接诊的多例肺炎病例与华...     6\n",
       "866  【#武汉已发现27例肺炎病例#】据武汉卫健委网站，近期部分医疗机构发现接诊的多例肺炎病例与华...     6\n",
       "972  【武汉卫健委：#已发现27例肺炎病例# 初步分析为病毒性肺炎，未出现人传人现象】近期部分医疗...     5\n",
       "652  【#武汉病毒性肺炎病例多为海鲜城经营户#】据湖北省相关部门消息，12月以来，武汉市持续开展流...     5\n",
       "958  【#武汉市卫健委通报肺炎疫情#：27例病例 7例病情严重】#武汉肺炎无明显人传人现象# 近期...     5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fear = weibo_df.sort_values(by='fear',ascending=False)\n",
    "fear[['text_cleaned', 'fear']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:24:33.554436Z",
     "start_time": "2020-06-12T07:24:33.542917Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【#武汉病毒性肺炎病例多为海鲜城经营户#】据湖北省相关部门消息，12月以来，武汉市持续开展流感及相关疾病监测，发现病毒性肺炎病例27例，均诊断为病毒性肺炎/肺部感染。其中7例病情危重，其余病例病情可控，2例病情好转拟近期出院。调查发现，此次肺炎病例大部分为华南海鲜城经营户。目前，相关病毒...全文： http://m.weibo.cn/2656274875/4455540560020855 \\u200b'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fear['text_cleaned'][652]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:24:48.413420Z",
     "start_time": "2020-06-12T07:24:48.386870Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>disgust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>#谢和弦吸毒##对毒品说不# 【吸毒毁容？这些吸毒者真实对比图给你真相（多图）】毒品能减肥能...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>【呼吸过速】是肺炎最敏感和特异性的体征。如果连呼吸平稳一点都不快，基本可以排除肺炎。\\n孩子...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>肺炎是症状。。。不明是不知道致病原因或者病毒。。。疑似病例是症状和某病类似，但没有疑似症状也...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>#武汉发现不明原因肺炎# 病人“陆续出现”？销声匿迹的SARS卷土重来？从广东变成华中？出门...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>回复@XXXX :辐射量很小的，不用担心。//@XXXX :张奶奶您好，小孩现在一岁五个月，...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_cleaned  disgust\n",
       "40   #谢和弦吸毒##对毒品说不# 【吸毒毁容？这些吸毒者真实对比图给你真相（多图）】毒品能减肥能...        8\n",
       "190  【呼吸过速】是肺炎最敏感和特异性的体征。如果连呼吸平稳一点都不快，基本可以排除肺炎。\\n孩子...        5\n",
       "304  肺炎是症状。。。不明是不知道致病原因或者病毒。。。疑似病例是症状和某病类似，但没有疑似症状也...        4\n",
       "741  #武汉发现不明原因肺炎# 病人“陆续出现”？销声匿迹的SARS卷土重来？从广东变成华中？出门...        4\n",
       "68   回复@XXXX :辐射量很小的，不用担心。//@XXXX :张奶奶您好，小孩现在一岁五个月，...        4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_type = 'disgust'\n",
    "df_test = weibo_df.sort_values(by=emotion_type,ascending=False)\n",
    "df_test[['text_cleaned', emotion_type]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T07:25:37.834889Z",
     "start_time": "2020-06-12T07:25:37.826430Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'肺炎是症状。。。不明是不知道致病原因或者病毒。。。疑似病例是症状和某病类似，但没有疑似症状也未必不是某种病毒。。。所以，这个辟谣是瞎扯淡，大家等cdc的官方消息吧。。。'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text_cleaned'][304]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "https://blog.csdn.net/weixin_38008864/article/details/103900840"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## cnsenti\n",
    "\n",
    "中文情感分析库(Chinese Sentiment)可对文本进行情绪分析、正负情感分析。\n",
    "\n",
    "- github地址 https://github.com/thunderhit/cnsenti\n",
    "- pypi地址 https://pypi.org/project/cnsenti/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T07:20:34.962273Z",
     "start_time": "2022-07-21T07:20:31.728487Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting cnsenti\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/64/91/06bff77081acf17c99bbc59aff7b06a834664b6bbe5c25bc250ce1f53911/cnsenti-0.0.7-py3-none-any.whl (395 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 KB\u001b[0m \u001b[31m892.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jieba in /opt/anaconda3/lib/python3.8/site-packages (from cnsenti) (0.42.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from cnsenti) (1.20.1)\n",
      "Installing collected packages: cnsenti\n",
      "Successfully installed cnsenti-0.0.7\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install -i https://pypi.tuna.tsinghua.edu.cn/simple cnsenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T07:21:32.910915Z",
     "start_time": "2022-07-21T07:21:31.361943Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/l6/ntr5b4610hx38gy0_2xp7ngh0000gn/T/jieba.cache\n",
      "Loading model cost 0.824 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 22, 'sentences': 2, 'pos': 4, 'neg': 0}\n"
     ]
    }
   ],
   "source": [
    "from cnsenti import Sentiment\n",
    "\n",
    "senti = Sentiment()\n",
    "test_text= '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心'\n",
    "result = senti.sentiment_count(test_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T07:21:46.164954Z",
     "start_time": "2022-07-21T07:21:46.151660Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': 22, 'sentences': 2, '好': 0, '乐': 4, '哀': 0, '怒': 0, '惧': 0, '恶': 0, '惊': 0}\n"
     ]
    }
   ],
   "source": [
    "from cnsenti import Emotion\n",
    "\n",
    "emotion = Emotion()\n",
    "test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心'\n",
    "result = emotion.emotion_count(test_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**sentiment_calculate(text)** belongs to the Sentiment class, which can calculate the emotional information of the chinese text more accurately. Compared with sentiment_count only counts the number of positive and negative sentiment words in the text, sentiment_calculate also considers\n",
    "\n",
    "- Is there a modifier of strength adverbs before emotional words\n",
    "- Is there an emotional semantic reversal effect of negative words before emotional words?\n",
    "for examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T07:23:53.508215Z",
     "start_time": "2022-07-21T07:23:53.488286Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_count {'words': 22, 'sentences': 2, 'pos': 4, 'neg': 0}\n",
      "sentiment_calculate {'sentences': 2, 'words': 22, 'pos': 27.0, 'neg': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from cnsenti import Sentiment\n",
    "\n",
    "senti = Sentiment()\n",
    "test_text = '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心'\n",
    "result1 = senti.sentiment_count(test_text)\n",
    "result2 = senti.sentiment_calculate(test_text)\n",
    "print('sentiment_count',result1)\n",
    "print('sentiment_calculate',result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## cntext\n",
    "\n",
    "https://github.com/hidadeng/cntext\n",
    "\n",
    "cntext is a text analysis package that provides semantic distance and semantic projection based on word embedding models. Besides,cntext also provides the traditional methods, such as word count , readability, document similarity, sentiment analysis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:43:09.507980Z",
     "start_time": "2022-07-21T11:43:06.411977Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: cntext in /opt/anaconda3/lib/python3.8/site-packages (1.7.9)\n",
      "Requirement already satisfied: numpy==1.20.0 in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (1.20.0)\n",
      "Requirement already satisfied: jieba in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (0.42.1)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (3.6.1)\n",
      "Requirement already satisfied: gensim==4.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (4.0.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (3.3.4)\n",
      "Requirement already satisfied: scikit-learn==1.0 in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (1.0)\n",
      "Requirement already satisfied: mittens in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (0.2)\n",
      "Requirement already satisfied: pyecharts in /opt/anaconda3/lib/python3.8/site-packages (from cntext) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim==4.0.0->cntext) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim==4.0.0->cntext) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn==1.0->cntext) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn==1.0->cntext) (2.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->cntext) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->cntext) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->cntext) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->cntext) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->cntext) (2.4.7)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.8/site-packages (from nltk->cntext) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk->cntext) (4.59.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk->cntext) (7.1.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from pyecharts->cntext) (2.11.3)\n",
      "Requirement already satisfied: prettytable in /opt/anaconda3/lib/python3.8/site-packages (from pyecharts->cntext) (3.3.0)\n",
      "Requirement already satisfied: simplejson in /opt/anaconda3/lib/python3.8/site-packages (from pyecharts->cntext) (3.17.6)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->cntext) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyecharts->cntext) (1.1.1)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.8/site-packages (from prettytable->pyecharts->cntext) (0.2.5)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade cntext  -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:51:23.838864Z",
     "start_time": "2022-07-21T11:50:32.711340Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Collecting pandas\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c8/85/8afe540bd0299c4d58f0a5b88acc49a8021804abe05a00d2cbc2fccde873/pandas-1.4.3-cp38-cp38-macosx_10_9_x86_64.whl (11.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m276.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\n",
      "    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\n",
      "Successfully installed pandas-1.4.3\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pandas  -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:52:32.466752Z",
     "start_time": "2022-07-21T11:52:25.722860Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting python-Levenshtein\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2a/dc/97f2b63ef0fa1fd78dcb7195aca577804f6b2b51e712516cc0e902a9a201/python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from python-Levenshtein) (52.0.0.post20210125)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp38-cp38-macosx_10_9_x86_64.whl size=80491 sha256=6a0876479d2777b80eec9fb1b73b1f5feb1079854b8b4c41d60dba3798030bbd\n",
      "  Stored in directory: /Users/chengjun/Library/Caches/pip/wheels/28/a5/92/bf15714fe87b46cdfefbba580ca70f86ee6392d27a1b501d4b\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein\n",
      "Successfully installed python-Levenshtein-0.12.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade python-Levenshtein  -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:52:50.879009Z",
     "start_time": "2022-07-21T11:52:50.874088Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package cntext:\n",
      "\n",
      "NAME\n",
      "    cntext\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    dictionary\n",
      "    mind\n",
      "    similarity\n",
      "    stats\n",
      "\n",
      "VERSION\n",
      "    1.7.9\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/lib/python3.8/site-packages/cntext/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cntext as ct\n",
    "\n",
    "help(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:56:12.150628Z",
     "start_time": "2022-07-21T11:56:12.146199Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DUTIR.pkl',\n",
       " 'HOWNET.pkl',\n",
       " 'Chinese_Loughran_McDonald_Financial_Sentiment.pkl',\n",
       " 'SentiWS.pkl',\n",
       " 'ChineseFinancialFormalUnformalSentiment.pkl',\n",
       " 'ANEW.pkl',\n",
       " 'LSD2015.pkl',\n",
       " 'NRC.pkl',\n",
       " 'geninqposneg.pkl',\n",
       " 'HuLiu.pkl',\n",
       " 'Loughran_McDonald_Financial_Sentiment.pkl',\n",
       " 'AFINN.pkl',\n",
       " 'ADV_CONJ.pkl',\n",
       " 'STOPWORDS.pkl',\n",
       " 'Concreteness.pkl',\n",
       " 'ChineseEmoBank.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.dict_pkl_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:55:38.007632Z",
     "start_time": "2022-07-21T11:55:37.997085Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger_num': 0,\n",
       " 'anticipation_num': 1,\n",
       " 'disgust_num': 0,\n",
       " 'fear_num': 0,\n",
       " 'joy_num': 1,\n",
       " 'negative_num': 0,\n",
       " 'positive_num': 1,\n",
       " 'sadness_num': 0,\n",
       " 'surprise_num': 0,\n",
       " 'trust_num': 1,\n",
       " 'stopword_num': 1,\n",
       " 'word_num': 5,\n",
       " 'sentence_num': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'What a happy day!'\n",
    "\n",
    "ct.sentiment(text=text,\n",
    "             diction=ct.load_pkl_dict('NRC.pkl')['NRC'],\n",
    "             lang='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:57:03.874083Z",
     "start_time": "2022-07-21T11:57:03.861742Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Referer': 'Brysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911',\n",
       " 'Desc': '语言具体性词典， 具体性计算应用案例可参考Packard, Grant, and Jonah Berger. \"How concrete language shapes customer satisfaction.\" *Journal of Consumer Research* 47, no. 5 (2021): 787-806.',\n",
       " 'Concreteness':                   word  valence\n",
       " 0          roadsweeper     4.85\n",
       " 1          traindriver     4.54\n",
       " 2                 tush     4.45\n",
       " 3            hairdress     3.93\n",
       " 4        pharmaceutics     3.77\n",
       " ...                ...      ...\n",
       " 39949         unenvied     1.21\n",
       " 39950     agnostically     1.20\n",
       " 39951  conceptualistic     1.18\n",
       " 39952  conventionalism     1.18\n",
       " 39953    essentialness     1.04\n",
       " \n",
       " [39954 rows x 2 columns]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.load_pkl_dict('concreteness.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:57:29.203436Z",
     "start_time": "2022-07-21T11:57:29.190140Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roadsweeper</td>\n",
       "      <td>4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traindriver</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tush</td>\n",
       "      <td>4.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hairdress</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pharmaceutics</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  valence\n",
       "0    roadsweeper     4.85\n",
       "1    traindriver     4.54\n",
       "2           tush     4.45\n",
       "3      hairdress     3.93\n",
       "4  pharmaceutics     3.77"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the concreteness.pkl dictionary file;  cntext version >=1.7.1\n",
    "concreteness_df = ct.load_pkl_dict('concreteness.pkl')['Concreteness']\n",
    "concreteness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:57:57.215210Z",
     "start_time": "2022-07-21T11:57:57.197092Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valence': 9.28, 'word_num': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply = \"I'll go look for that\"\n",
    "\n",
    "score=ct.sentiment_by_valence(text=reply, \n",
    "                              diction=concreteness_df, \n",
    "                              lang='english')\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:02:53.846511Z",
     "start_time": "2022-07-21T12:02:53.746320Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concreteness Score: 1.86 | Example-0: I'll go look for that\n",
      "Concreteness Score: 1.86 | Example-1: I'll go search for that\n",
      "Concreteness Score: 2.21 | Example-2: I'll go search for that top\n",
      "Concreteness Score: 2.04 | Example-3: I'll go search for that t-shirt\n",
      "Concreteness Score: 2.37 | Example-4: I'll go look for that t-shirt in grey\n",
      "Concreteness Score: 2.37 | Example-5: I'll go search for that t-shirt in grey\n"
     ]
    }
   ],
   "source": [
    "employee_replys = [\"I'll go look for that\",\n",
    "                   \"I'll go search for that\",\n",
    "                   \"I'll go search for that top\",\n",
    "                   \"I'll go search for that t-shirt\",\n",
    "                   \"I'll go look for that t-shirt in grey\",\n",
    "                   \"I'll go search for that t-shirt in grey\"]\n",
    "\n",
    "for idx, reply in enumerate(employee_replys):\n",
    "    score=ct.sentiment_by_valence(text=reply, \n",
    "                                  diction=concreteness_df, \n",
    "                                  lang='english')\n",
    "    \n",
    "    \n",
    "    template = \"Concreteness Score: {score:.2f} | Example-{idx}: {example}\"\n",
    "    print(template.format(score=score['valence']/score['word_num'], \n",
    "                          idx=idx, \n",
    "                          example=reply))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:11:05.599905Z",
     "start_time": "2022-07-21T12:10:03.155453Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/4:...Preprocess   Corpus ...\n",
      "Step 2/4:...Collect co-occurrency information ...\n",
      "Step 3/4:...Calculate   mutual information ...\n",
      "Step 4/4:...Save    candidate words ...\n",
      "Finish! used 62.41 s\n"
     ]
    }
   ],
   "source": [
    "# This module is used to build or expand the vocabulary (dictionary), including\n",
    "\n",
    "# SoPmi Co-occurrence algorithm to extend vocabulary (dictionary), Only support chinese\n",
    "# W2VModels using word2vec to extend vocabulary (dictionary), support english & chinese\n",
    "\n",
    "import os\n",
    "\n",
    "sopmier = ct.SoPmi(cwd=os.getcwd(),\n",
    "                   #raw corpus data，txt file.only support chinese data now.\n",
    "                   input_txt_file='/Users/chengjun/GitHub/cntext/examples/data/sopmi_corpus.txt', \n",
    "                   #muanually selected seed words\n",
    "                   seedword_txt_file='/Users/chengjun/GitHub/cntext/examples/data/sopmi_seed_words.txt', #人工标注的初始种子词\n",
    "                   )   \n",
    "\n",
    "sopmier.sopmi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:12:14.842526Z",
     "start_time": "2022-07-21T12:12:14.837856Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chengjun/GitHub/css/notebook'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:14:13.096781Z",
     "start_time": "2022-07-21T12:12:50.789549Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/4:...Preprocess   corpus ...\n",
      "Step 2/4:...Train  word2vec model\n",
      "            used   72 s\n",
      "Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...\n",
      "Step 4/4 Finish! Used 82 s\n",
      "Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...\n",
      "Step 4/4 Finish! Used 82 s\n",
      "Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...\n",
      "Step 4/4 Finish! Used 82 s\n",
      "Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...\n",
      "Step 4/4 Finish! Used 82 s\n",
      "Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...\n",
      "Step 4/4 Finish! Used 82 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#init W2VModels, corpus data w2v_corpus.txt\n",
    "model = ct.W2VModels(cwd=os.getcwd(), lang='english')  \n",
    "model.train(input_txt_file='/Users/chengjun/GitHub/cntext/examples/data/w2v_corpus.txt')\n",
    "\n",
    "\n",
    "#According to the seed word, filter out the top 100 words that are most similar to each category words\n",
    "model.find(seedword_txt_file='/Users/chengjun/GitHub/cntext/examples/data/w2v_seeds/integrity.txt', \n",
    "           topn=100)\n",
    "model.find(seedword_txt_file='/Users/chengjun/GitHub/cntext/examples/data/w2v_seeds/innovation.txt', \n",
    "           topn=100)\n",
    "model.find(seedword_txt_file='/Users/chengjun/GitHub/cntext/examples/data/w2v_seeds/quality.txt', \n",
    "           topn=100)\n",
    "model.find(seedword_txt_file='/Users/chengjun/GitHub/cntext/examples/data/w2v_seeds/respect.txt', \n",
    "           topn=100)\n",
    "model.find(seedword_txt_file='/Users/chengjun/GitHub/cntext/examples/data/w2v_seeds/teamwork.txt', \n",
    "           topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:15:53.463147Z",
     "start_time": "2022-07-21T12:15:53.454324Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.315214  ,  0.8820462 ,  1.1123291 ,  0.57502085, -0.4536534 ,\n",
       "        0.04593553,  1.7608879 ,  1.8781638 ,  0.62472534, -0.40170938,\n",
       "       -0.74852383,  0.20071138, -0.1615613 , -1.1544902 , -1.8266941 ,\n",
       "        0.07904029,  0.04081295, -0.30807   , -0.25948036,  0.7637631 ,\n",
       "        0.30233267,  0.02158124, -0.4548134 ,  0.22692135, -0.26944858,\n",
       "       -0.24163176,  1.2302433 ,  0.70777947,  1.1255033 , -0.17567617,\n",
       "       -0.7234768 , -1.0653226 , -0.09055816, -1.46483   ,  0.4885035 ,\n",
       "       -0.9827379 ,  0.95400816, -0.13374516, -0.52517796, -0.23480052,\n",
       "        0.62640554, -0.5553755 , -0.83863366,  1.3104644 , -0.07400908,\n",
       "        1.1336509 ,  0.32375774,  0.354964  ,  0.10843342, -0.54200864,\n",
       "       -0.8003504 , -0.7266025 ,  1.4322623 , -0.8662505 , -0.7207146 ,\n",
       "        0.16009226,  1.746251  ,  1.7912021 ,  1.9516977 ,  0.7451547 ,\n",
       "       -0.43844843, -0.30046785,  1.7179738 ,  0.8191196 , -0.41387284,\n",
       "       -0.26602927, -1.3589116 , -0.0668382 , -0.02384867, -0.5849235 ,\n",
       "        0.55971706,  0.96517783,  1.5546738 ,  0.7545263 ,  1.4992219 ,\n",
       "        1.0422503 ,  0.51255804, -0.67980427, -0.8455454 ,  1.3597207 ,\n",
       "       -0.31741807, -1.42675   , -0.04144925, -0.6972174 , -0.11606634,\n",
       "        1.8798316 ,  0.11557695, -1.1349543 ,  1.3116952 ,  0.3919832 ,\n",
       "        0.59277475, -0.45137736,  0.93078315,  0.45508283,  0.8245617 ,\n",
       "        0.02893427,  1.0617537 , -0.78552085,  0.43378317,  2.971067  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_model = KeyedVectors.load(\"/Users/chengjun/GitHub/cntext/examples/output/w2v_candi_words/w2v.model\")\n",
    "w2v_model.get_vector('company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:16:16.087154Z",
     "start_time": "2022-07-21T12:16:16.079654Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('execution', 0.8257297873497009),\n",
       " ('capabilities', 0.8198432326316833),\n",
       " ('continue_focus', 0.8191850781440735),\n",
       " ('dsd_system', 0.8175386786460876),\n",
       " ('continue_expand', 0.8135412931442261),\n",
       " ('emphasis', 0.8049392700195312),\n",
       " ('informatics', 0.8048107624053955),\n",
       " ('continue_drive', 0.8032825589179993),\n",
       " ('technologies', 0.8026941418647766),\n",
       " ('dsd', 0.8008151650428772)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('innovation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:16:25.722448Z",
     "start_time": "2022-07-21T12:16:25.717226Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10703183, -0.5493035 ,  1.0623612 , -0.43180484, -0.9257197 ,\n",
       "       -1.0833972 , -0.07150532,  1.8700444 ,  0.26021707,  0.04272432,\n",
       "       -0.93499535,  0.05657623,  1.1683912 ,  0.3376583 ,  0.49347457,\n",
       "       -0.7696766 , -0.91470116, -0.66640836,  1.2935071 , -0.6879933 ,\n",
       "        0.01370158, -0.07028562, -1.3170564 , -0.4464321 , -0.08877528,\n",
       "        0.634105  , -0.9797471 ,  1.221016  , -0.5741402 , -0.18807778,\n",
       "       -1.5563715 ,  0.14269993,  0.53213865, -0.16015887, -0.16328476,\n",
       "       -0.03469868,  0.8123352 , -0.89830345, -0.4654071 ,  0.59473026,\n",
       "       -0.01235237, -0.16233891,  0.0882808 ,  1.2530324 ,  0.63678694,\n",
       "       -0.4859363 ,  0.63639694,  0.5577657 ,  1.1487194 ,  1.2548769 ,\n",
       "        0.08172881,  1.2036918 ,  0.31136826, -0.33539033, -0.73395115,\n",
       "       -0.35624295,  0.00873698, -0.7198372 ,  0.09266879,  0.0259969 ,\n",
       "        0.5248478 ,  1.0185517 , -0.3534738 , -0.11561136, -0.64694124,\n",
       "       -0.13806236, -0.7061978 ,  0.4576997 , -0.9011815 ,  0.47045934,\n",
       "        0.7067756 ,  0.08959749,  0.31015027,  0.284999  ,  1.887349  ,\n",
       "        0.38284793,  0.35984853, -0.413301  , -0.2978032 , -0.13867766,\n",
       "        0.6174871 ,  0.55524594, -0.1517662 ,  0.05696797,  0.06463829,\n",
       "       -0.0501307 , -0.7806092 ,  0.2816824 ,  0.19331798,  0.2766436 ,\n",
       "        1.5176852 ,  0.01909402, -0.2754046 ,  0.41802418,  0.44312304,\n",
       "        1.2473223 ,  1.5042527 , -1.0180748 ,  0.35682076, -0.550147  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.get_vector('innovation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:04:27.409744Z",
     "start_time": "2022-07-21T12:04:27.397148Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>bus</th>\n",
       "      <th>by</th>\n",
       "      <th>day</th>\n",
       "      <th>every</th>\n",
       "      <th>go</th>\n",
       "      <th>i</th>\n",
       "      <th>night</th>\n",
       "      <th>school</th>\n",
       "      <th>theatre</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bus</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theatre</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         .  bus  by  day  every  go  i  night  school  theatre  to\n",
       ".        0    1   1    0      0   0  0      0       0        0   0\n",
       "bus      1    0   2    1      0   0  0      1       0        0   0\n",
       "by       1    2   0    1      2   0  0      1       0        0   0\n",
       "day      0    1   1    0      1   0  0      0       1        0   0\n",
       "every    0    0   2    1      0   0  0      1       1        1   2\n",
       "go       0    0   0    0      0   0  2      0       1        1   2\n",
       "i        0    0   0    0      0   2  0      0       0        0   2\n",
       "night    0    1   1    0      1   0  0      0       0        1   0\n",
       "school   0    0   0    1      1   1  0      0       0        0   1\n",
       "theatre  0    0   0    0      1   1  0      1       0        0   1\n",
       "to       0    0   0    0      2   2  2      0       1        1   0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"I go to school every day by bus .\",\n",
    "         \"i go to theatre every night by bus\"]\n",
    "\n",
    "ct.co_occurrence_matrix(documents, \n",
    "                        window_size=2, \n",
    "                        lang='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:17:06.028525Z",
     "start_time": "2022-07-21T12:17:06.018573Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n",
      "0.50\n",
      "1.00\n",
      "0.90\n"
     ]
    }
   ],
   "source": [
    "text1 = 'Programming is fun!'\n",
    "text2 = 'Programming is interesting!'\n",
    "\n",
    "print(ct.cosine_sim(text1, text2))\n",
    "print(ct.jaccard_sim(text1, text2))\n",
    "print(ct.minedit_sim(text1, text2))\n",
    "print(ct.simple_sim(text1, text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:21:15.953561Z",
     "start_time": "2022-07-21T12:21:15.485112Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model of /Users/chengjun/GitHub/cntext/examples/output/Glove/brown_corpus_w2v.txt\n",
      "Load successfully, used 0.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download glove_w2v.6B.100d.txt from google Driver\n",
    "# https://drive.google.com/file/d/1tuQB9PDx42z67ScEQrg650aDTYPz-elJ/view\n",
    "#Note: this is a word2vec format model\n",
    "# tm = ct.Text2Mind(w2v_model_path='glove_w2v.6B.100d.txt')\n",
    "tm = ct.Text2Mind(w2v_model_path='/Users/chengjun/GitHub/cntext/examples/output/Glove/brown_corpus_w2v.txt')\n",
    "\n",
    "\n",
    "engineer = ['program', 'software', 'computer']\n",
    "mans =  [\"man\", \"he\", \"him\"]\n",
    "womans = [\"woman\", \"she\", \"her\"]\n",
    "\n",
    "\n",
    "tm.sematic_distance(words=animals, \n",
    "                    c_words1=mans, \n",
    "                    c_words2=womans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T12:21:31.826006Z",
     "start_time": "2022-07-21T12:21:31.818649Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horse', -2.52), ('cat', -2.03), ('pig', -1.75), ('mouse', -1.64)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = ['mouse', 'cat', 'horse',  'pig']#, 'whale']\n",
    "small_words = [\"small\", \"little\", \"tiny\"]\n",
    "large_words = [\"large\", \"big\", \"huge\"]\n",
    "\n",
    "tm.sematic_projection(words=animals, \n",
    "                      c_words1=small_words, \n",
    "                      c_words2=large_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:52:55.792131Z",
     "start_time": "2022-07-21T11:52:55.762508Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Referer-1': 'Lee, Lung-Hao, Jian-Hong Li, and Liang-Chih Yu. \"Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis.\" Transactions on Asian and Low-Resource Language Information Processing 21, no. 4 (2022): 1-18.',\n",
       " 'Referer-2': 'Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang, Yunchao He, Jun Hu, K. Robert Lai, and Xuejie Zhang. 2016. \"Building Chinese affective resources in valence-arousal dimensions. In Proceedings of NAACL/HLT-16, pages 540-545.',\n",
       " 'Desc': 'Chinese Sentiment Dictionary, includes 「valence」「arousal」. In cntext, we only take Chinese valence-arousal words (CVAW, single word) into account, ignore CVAP, CVAS, CVAT.',\n",
       " 'ChineseEmoBank':       word  valence  arousal\n",
       " 0     不可思议      5.4      7.2\n",
       " 1       不平      3.6      5.8\n",
       " 2       不甘      3.2      6.4\n",
       " 3       不安      3.8      5.4\n",
       " 4       不利      3.6      5.6\n",
       " ...    ...      ...      ...\n",
       " 5505    黏闷      2.8      5.6\n",
       " 5506    黏腻      2.7      5.8\n",
       " 5507    艳丽      5.8      4.5\n",
       " 5508    苗条      6.7      3.8\n",
       " 5509    修长      7.0      4.5\n",
       " \n",
       " [5510 rows x 3 columns]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.load_pkl_dict('ChineseEmoBank.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:53:06.871093Z",
     "start_time": "2022-07-21T11:53:06.857225Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>不可思议</td>\n",
       "      <td>5.4</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>不平</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>不甘</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>不安</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>不利</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>黏闷</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>黏腻</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>艳丽</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>苗条</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5509</th>\n",
       "      <td>修长</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5510 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  valence  arousal\n",
       "0     不可思议      5.4      7.2\n",
       "1       不平      3.6      5.8\n",
       "2       不甘      3.2      6.4\n",
       "3       不安      3.8      5.4\n",
       "4       不利      3.6      5.6\n",
       "...    ...      ...      ...\n",
       "5505    黏闷      2.8      5.6\n",
       "5506    黏腻      2.7      5.8\n",
       "5507    艳丽      5.8      4.5\n",
       "5508    苗条      6.7      3.8\n",
       "5509    修长      7.0      4.5\n",
       "\n",
       "[5510 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diction_df = ct.load_pkl_dict('ChineseEmoBank.pkl')['ChineseEmoBank']\n",
    "diction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:53:10.588940Z",
     "start_time": "2022-07-21T11:53:09.881112Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/l6/ntr5b4610hx38gy0_2xp7ngh0000gn/T/jieba.cache\n",
      "Loading model cost 0.677 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'valence': 14.8, 'arousal': 24.8, 'word_num': 13}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '很多车主抱怨新车怠速抖动严重---冷车时更严重。'\n",
    "\n",
    "ct.sentiment_by_weight(text = text, \n",
    "                       diction = diction_df,\n",
    "                       params = ['valence', 'arousal'],\n",
    "                       lang = 'chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T11:54:27.162717Z",
     "start_time": "2022-07-21T11:54:27.158700Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- valence是句子中各个chinese_emobank词valence得分的加总。\n",
    "- arousal是句子中各个chinese_emobank词arousal得分的加总。\n",
    "- word_num是句子中的词语数(含标点符号)，短文本的情况下，word_num会不太准确，长文本情况下无限接近真实词语数。\n",
    "\n",
    "需要注意，文本越长，valence和arousal指标应该会越大。使用这两个指标时，需要结合word_num进行均值处理，即\n",
    "\n",
    "- Valence = valence/word_num\n",
    "- Arousal = arousal/word_num\n",
    "这里未做均值处理，尽量保留文本的原始信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/chengjun2.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

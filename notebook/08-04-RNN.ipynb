{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "# Sequence Modeling: Recurrent and Recursive Nets\n",
    "---\n",
    "---\n",
    "\n",
    "![image.png](img/chengjun.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Recurrent neural networks**, or RNNs (Rumelhart et al., 1986a), are a family of neural networks for processing sequential data. \n",
    "\n",
    "Much as a convolutional network is a neural network that is specialized for processing a grid of values $X$ such as an image, a recurrent neural network is a neural network that is specialized for processing a sequence of values $x^{(1)}$, . . . , $x^{(τ)}$. \n",
    "\n",
    "\n",
    "Most recurrent networks can also process sequences of *variable length*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**RNN Applications: series of data**\n",
    "\n",
    "- Time series prediction\n",
    "- Language modeling (text generation)\n",
    "- Text sentiment analysis \n",
    "- Named entity recognition\n",
    "- Translation\n",
    "- Speech recognition \n",
    "- Anomaly detection in time series \n",
    "- Music composition\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl43.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn.gif](img/rnn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl44.png)\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RNN in a Nutshell\n",
    "\n",
    "![image.png](img/dl47.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:42:33.912449Z",
     "start_time": "2021-08-08T03:42:32.179866Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# One hot encoding for each char in 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:43:04.565161Z",
     "start_time": "2021-08-08T03:43:04.557772Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
    "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
    "\n",
    "# (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "hidden = Variable(torch.randn(1, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:44:19.681351Z",
     "start_time": "2021-08-08T03:44:19.657797Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence input size torch.Size([1, 5, 4]) out size torch.Size([1, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "inputs = inputs.view(1, 5, -1)\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"sequence input size\", inputs.size(), \"out size\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:45:10.351689Z",
     "start_time": "2021-08-08T03:45:10.342799Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input size torch.Size([3, 5, 4]) out size torch.Size([3, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
    "# 3 batches 'hello', 'eolll', 'lleel'\n",
    "# rank = (3, 5, 4)\n",
    "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
    "                                [e, o, l, l, l],\n",
    "                                [l, l, e, e, l]]))\n",
    "\n",
    "# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
    "hidden = Variable(torch.randn(1, 3, 2))\n",
    "\n",
    "# Propagate input through RNN\n",
    "# Input: (batch, seq_len, input_size) when batch_first=True\n",
    "# B x S x I\n",
    "out, hidden = cell(inputs, hidden)\n",
    "print(\"batch input size\", inputs.size(), \"out size\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:45:14.820226Z",
     "start_time": "2021-08-08T03:45:14.802268Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]],\n",
       "\n",
       "        [[0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:45:22.947640Z",
     "start_time": "2021-08-08T03:45:22.929000Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4814,  0.2471],\n",
       "         [ 0.5347, -0.7445],\n",
       "         [-0.1018, -0.1504]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:45:28.583283Z",
     "start_time": "2021-08-08T03:45:28.576295Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9635, -0.7152],\n",
       "         [ 0.6888,  0.4281],\n",
       "         [-0.0014, -0.0229],\n",
       "         [ 0.1338, -0.5729],\n",
       "         [ 0.4814,  0.2471]],\n",
       "\n",
       "        [[ 0.6749,  0.3051],\n",
       "         [ 0.0923,  0.7774],\n",
       "         [-0.3299, -0.1518],\n",
       "         [ 0.1451, -0.7179],\n",
       "         [ 0.5347, -0.7445]],\n",
       "\n",
       "        [[-0.6955,  0.2545],\n",
       "         [-0.1804, -0.7042],\n",
       "         [ 0.5372, -0.1592],\n",
       "         [ 0.3898,  0.4898],\n",
       "         [-0.1018, -0.1504]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl48.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl49.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl50.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:47:41.988571Z",
     "start_time": "2021-08-08T03:47:41.976267Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "#            0    1    2    3    4\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [0, 1, 0, 2, 3, 3]   # hihell\n",
    "one_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n",
    "                  [0, 1, 0, 0, 0],  # 1\n",
    "                  [0, 0, 1, 0, 0],  # 2\n",
    "                  [0, 0, 0, 1, 0],  # 3\n",
    "                  [0, 0, 0, 0, 1]]  # 4\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:48:19.550071Z",
     "start_time": "2021-08-08T03:48:19.540423Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2, 3, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the RNN. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 1  #Note: One by one\n",
    "num_layers = 1  # one-layer rnn\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:49:25.862970Z",
     "start_time": "2021-08-08T03:49:25.856073Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size, batch_first=True)\n",
    "    def forward(self, hidden, x):\n",
    "        # Reshape input (batch first)\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        return hidden, out.view(-1, num_classes)\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:49:30.778971Z",
     "start_time": "2021-08-08T03:49:30.770671Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RNN model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T03:49:40.247015Z",
     "start_time": "2021-08-08T03:49:39.666190Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted string: llllll, epoch: 1, loss: 10.155\n",
      "predicted string: llllll, epoch: 2, loss: 9.137\n",
      "predicted string: llllll, epoch: 3, loss: 8.355\n",
      "predicted string: llllll, epoch: 4, loss: 7.577\n",
      "predicted string: llllll, epoch: 5, loss: 6.876\n",
      "predicted string: lhelll, epoch: 6, loss: 6.327\n",
      "predicted string: ihelll, epoch: 7, loss: 6.014\n",
      "predicted string: ihelll, epoch: 8, loss: 5.787\n",
      "predicted string: ihelll, epoch: 9, loss: 5.477\n",
      "predicted string: ihelll, epoch: 10, loss: 5.274\n",
      "predicted string: ihelll, epoch: 11, loss: 5.041\n",
      "predicted string: ihello, epoch: 12, loss: 4.827\n",
      "predicted string: ihello, epoch: 13, loss: 4.676\n",
      "predicted string: ihello, epoch: 14, loss: 4.550\n",
      "predicted string: ihello, epoch: 15, loss: 4.430\n",
      "predicted string: ihello, epoch: 16, loss: 4.305\n",
      "predicted string: ihello, epoch: 17, loss: 4.164\n",
      "predicted string: ihelll, epoch: 18, loss: 4.003\n",
      "predicted string: ihelll, epoch: 19, loss: 3.860\n",
      "predicted string: ihelll, epoch: 20, loss: 3.879\n",
      "predicted string: ihelll, epoch: 21, loss: 3.768\n",
      "predicted string: ihelll, epoch: 22, loss: 3.642\n",
      "predicted string: ihelll, epoch: 23, loss: 3.599\n",
      "predicted string: ihello, epoch: 24, loss: 3.577\n",
      "predicted string: ihello, epoch: 25, loss: 3.544\n",
      "predicted string: ihello, epoch: 26, loss: 3.498\n",
      "predicted string: ihello, epoch: 27, loss: 3.439\n",
      "predicted string: ihello, epoch: 28, loss: 3.371\n",
      "predicted string: ihello, epoch: 29, loss: 3.303\n",
      "predicted string: ihello, epoch: 30, loss: 3.240\n",
      "predicted string: ihello, epoch: 31, loss: 3.162\n",
      "predicted string: ihello, epoch: 32, loss: 3.147\n",
      "predicted string: ihello, epoch: 33, loss: 3.178\n",
      "predicted string: ihello, epoch: 34, loss: 3.116\n",
      "predicted string: ihello, epoch: 35, loss: 3.042\n",
      "predicted string: ihello, epoch: 36, loss: 3.020\n",
      "predicted string: ihello, epoch: 37, loss: 3.015\n",
      "predicted string: ihello, epoch: 38, loss: 2.998\n",
      "predicted string: ihello, epoch: 39, loss: 2.977\n",
      "predicted string: ihello, epoch: 40, loss: 2.966\n",
      "predicted string: ihello, epoch: 41, loss: 2.961\n",
      "predicted string: ihello, epoch: 42, loss: 2.950\n",
      "predicted string: ihello, epoch: 43, loss: 2.930\n",
      "predicted string: ihello, epoch: 44, loss: 2.904\n",
      "predicted string: ihello, epoch: 45, loss: 2.888\n",
      "predicted string: ihello, epoch: 46, loss: 2.888\n",
      "predicted string: ihello, epoch: 47, loss: 2.879\n",
      "predicted string: ihello, epoch: 48, loss: 2.860\n",
      "predicted string: ihello, epoch: 49, loss: 2.857\n",
      "predicted string: ihello, epoch: 50, loss: 2.859\n",
      "predicted string: ihello, epoch: 51, loss: 2.852\n",
      "predicted string: ihello, epoch: 52, loss: 2.840\n",
      "predicted string: ihello, epoch: 53, loss: 2.834\n",
      "predicted string: ihello, epoch: 54, loss: 2.834\n",
      "predicted string: ihello, epoch: 55, loss: 2.824\n",
      "predicted string: ihello, epoch: 56, loss: 2.817\n",
      "predicted string: ihello, epoch: 57, loss: 2.817\n",
      "predicted string: ihello, epoch: 58, loss: 2.814\n",
      "predicted string: ihello, epoch: 59, loss: 2.808\n",
      "predicted string: ihello, epoch: 60, loss: 2.805\n",
      "predicted string: ihello, epoch: 61, loss: 2.805\n",
      "predicted string: ihello, epoch: 62, loss: 2.801\n",
      "predicted string: ihello, epoch: 63, loss: 2.796\n",
      "predicted string: ihello, epoch: 64, loss: 2.795\n",
      "predicted string: ihello, epoch: 65, loss: 2.793\n",
      "predicted string: ihello, epoch: 66, loss: 2.789\n",
      "predicted string: ihello, epoch: 67, loss: 2.786\n",
      "predicted string: ihello, epoch: 68, loss: 2.786\n",
      "predicted string: ihello, epoch: 69, loss: 2.783\n",
      "predicted string: ihello, epoch: 70, loss: 2.780\n",
      "predicted string: ihello, epoch: 71, loss: 2.780\n",
      "predicted string: ihello, epoch: 72, loss: 2.778\n",
      "predicted string: ihello, epoch: 73, loss: 2.776\n",
      "predicted string: ihello, epoch: 74, loss: 2.775\n",
      "predicted string: ihello, epoch: 75, loss: 2.774\n",
      "predicted string: ihello, epoch: 76, loss: 2.772\n",
      "predicted string: ihello, epoch: 77, loss: 2.770\n",
      "predicted string: ihello, epoch: 78, loss: 2.769\n",
      "predicted string: ihello, epoch: 79, loss: 2.768\n",
      "predicted string: ihello, epoch: 80, loss: 2.766\n",
      "predicted string: ihello, epoch: 81, loss: 2.765\n",
      "predicted string: ihello, epoch: 82, loss: 2.764\n",
      "predicted string: ihello, epoch: 83, loss: 2.763\n",
      "predicted string: ihello, epoch: 84, loss: 2.762\n",
      "predicted string: ihello, epoch: 85, loss: 2.761\n",
      "predicted string: ihello, epoch: 86, loss: 2.759\n",
      "predicted string: ihello, epoch: 87, loss: 2.759\n",
      "predicted string: ihello, epoch: 88, loss: 2.758\n",
      "predicted string: ihello, epoch: 89, loss: 2.757\n",
      "predicted string: ihello, epoch: 90, loss: 2.756\n",
      "predicted string: ihello, epoch: 91, loss: 2.755\n",
      "predicted string: ihello, epoch: 92, loss: 2.754\n",
      "predicted string: ihello, epoch: 93, loss: 2.753\n",
      "predicted string: ihello, epoch: 94, loss: 2.752\n",
      "predicted string: ihello, epoch: 95, loss: 2.751\n",
      "predicted string: ihello, epoch: 96, loss: 2.750\n",
      "predicted string: ihello, epoch: 97, loss: 2.750\n",
      "predicted string: ihello, epoch: 98, loss: 2.749\n",
      "predicted string: ihello, epoch: 99, loss: 2.748\n",
      "predicted string: ihello, epoch: 100, loss: 2.747\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    sys.stdout.write(\"predicted string: \")\n",
    "    for input, label in zip(inputs, labels):\n",
    "        # print(input.size(), label.size())\n",
    "        hidden, output = model(hidden, input)\n",
    "        val, idx = output.max(1)\n",
    "        sys.stdout.write(idx2char[idx.data[0]])\n",
    "        #label_one_hot = one_hot_lookup[label]\n",
    "        loss += criterion(output, label.view(-1))\n",
    "\n",
    "    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unfolding one to n sequences\n",
    "\n",
    "![image.png](img/dl51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:25:01.865952Z",
     "start_time": "2020-06-01T08:25:01.860982Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "\n",
    "# Note: x_one_hot is changed to 1, 6, 5\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:25:02.313130Z",
     "start_time": "2020-06-01T08:25:02.309602Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # Note: |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T02:24:12.800185Z",
     "start_time": "2020-08-14T02:24:12.794329Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        # Reshape input\n",
    "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "        out, _ = self.rnn(x, h_0) \n",
    "        return out.view(-1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:25:03.394452Z",
     "start_time": "2020-06-01T08:25:03.389212Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RNN model\n",
    "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
    "print(rnn)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T08:25:04.341055Z",
     "start_time": "2020-06-01T08:25:04.173109Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.544\n",
      "Predicted string:  lellll\n",
      "epoch: 2, loss: 1.315\n",
      "Predicted string:  lillll\n",
      "epoch: 3, loss: 1.169\n",
      "Predicted string:  lieloo\n",
      "epoch: 4, loss: 1.041\n",
      "Predicted string:  liello\n",
      "epoch: 5, loss: 0.942\n",
      "Predicted string:  lhello\n",
      "epoch: 6, loss: 0.865\n",
      "Predicted string:  ihello\n",
      "epoch: 7, loss: 0.801\n",
      "Predicted string:  ihelll\n",
      "epoch: 8, loss: 0.752\n",
      "Predicted string:  ihelll\n",
      "epoch: 9, loss: 0.721\n",
      "Predicted string:  ihelll\n",
      "epoch: 10, loss: 0.693\n",
      "Predicted string:  ihelll\n",
      "epoch: 11, loss: 0.667\n",
      "Predicted string:  ihelll\n",
      "epoch: 12, loss: 0.648\n",
      "Predicted string:  ihelll\n",
      "epoch: 13, loss: 0.631\n",
      "Predicted string:  ihelll\n",
      "epoch: 14, loss: 0.617\n",
      "Predicted string:  ihelll\n",
      "epoch: 15, loss: 0.605\n",
      "Predicted string:  ihelll\n",
      "epoch: 16, loss: 0.595\n",
      "Predicted string:  ihelll\n",
      "epoch: 17, loss: 0.588\n",
      "Predicted string:  ihelll\n",
      "epoch: 18, loss: 0.582\n",
      "Predicted string:  ihelll\n",
      "epoch: 19, loss: 0.574\n",
      "Predicted string:  ihelll\n",
      "epoch: 20, loss: 0.568\n",
      "Predicted string:  ihelll\n",
      "epoch: 21, loss: 0.564\n",
      "Predicted string:  ihelll\n",
      "epoch: 22, loss: 0.561\n",
      "Predicted string:  ihelll\n",
      "epoch: 23, loss: 0.557\n",
      "Predicted string:  ihelll\n",
      "epoch: 24, loss: 0.554\n",
      "Predicted string:  ihelll\n",
      "epoch: 25, loss: 0.552\n",
      "Predicted string:  ihelll\n",
      "epoch: 26, loss: 0.550\n",
      "Predicted string:  ihelll\n",
      "epoch: 27, loss: 0.547\n",
      "Predicted string:  ihelll\n",
      "epoch: 28, loss: 0.545\n",
      "Predicted string:  ihelll\n",
      "epoch: 29, loss: 0.543\n",
      "Predicted string:  ihelll\n",
      "epoch: 30, loss: 0.542\n",
      "Predicted string:  ihelll\n",
      "epoch: 31, loss: 0.540\n",
      "Predicted string:  ihelll\n",
      "epoch: 32, loss: 0.539\n",
      "Predicted string:  ihelll\n",
      "epoch: 33, loss: 0.538\n",
      "Predicted string:  ihelll\n",
      "epoch: 34, loss: 0.537\n",
      "Predicted string:  ihelll\n",
      "epoch: 35, loss: 0.536\n",
      "Predicted string:  ihelll\n",
      "epoch: 36, loss: 0.535\n",
      "Predicted string:  ihelll\n",
      "epoch: 37, loss: 0.535\n",
      "Predicted string:  ihelll\n",
      "epoch: 38, loss: 0.534\n",
      "Predicted string:  ihelll\n",
      "epoch: 39, loss: 0.533\n",
      "Predicted string:  ihelll\n",
      "epoch: 40, loss: 0.533\n",
      "Predicted string:  ihelll\n",
      "epoch: 41, loss: 0.533\n",
      "Predicted string:  ihelll\n",
      "epoch: 42, loss: 0.532\n",
      "Predicted string:  ihelll\n",
      "epoch: 43, loss: 0.532\n",
      "Predicted string:  ihelll\n",
      "epoch: 44, loss: 0.531\n",
      "Predicted string:  ihelll\n",
      "epoch: 45, loss: 0.531\n",
      "Predicted string:  ihelll\n",
      "epoch: 46, loss: 0.531\n",
      "Predicted string:  ihelll\n",
      "epoch: 47, loss: 0.530\n",
      "Predicted string:  ihelll\n",
      "epoch: 48, loss: 0.530\n",
      "Predicted string:  ihelll\n",
      "epoch: 49, loss: 0.530\n",
      "Predicted string:  ihelll\n",
      "epoch: 50, loss: 0.529\n",
      "Predicted string:  ihelll\n",
      "epoch: 51, loss: 0.529\n",
      "Predicted string:  ihelll\n",
      "epoch: 52, loss: 0.529\n",
      "Predicted string:  ihelll\n",
      "epoch: 53, loss: 0.529\n",
      "Predicted string:  ihelll\n",
      "epoch: 54, loss: 0.529\n",
      "Predicted string:  ihelll\n",
      "epoch: 55, loss: 0.528\n",
      "Predicted string:  ihelll\n",
      "epoch: 56, loss: 0.528\n",
      "Predicted string:  ihelll\n",
      "epoch: 57, loss: 0.528\n",
      "Predicted string:  ihelll\n",
      "epoch: 58, loss: 0.528\n",
      "Predicted string:  ihelll\n",
      "epoch: 59, loss: 0.528\n",
      "Predicted string:  ihelll\n",
      "epoch: 60, loss: 0.528\n",
      "Predicted string:  ihelll\n",
      "epoch: 61, loss: 0.527\n",
      "Predicted string:  ihelll\n",
      "epoch: 62, loss: 0.527\n",
      "Predicted string:  ihelll\n",
      "epoch: 63, loss: 0.527\n",
      "Predicted string:  ihelll\n",
      "epoch: 64, loss: 0.527\n",
      "Predicted string:  ihelll\n",
      "epoch: 65, loss: 0.527\n",
      "Predicted string:  ihelll\n",
      "epoch: 66, loss: 0.527\n",
      "Predicted string:  ihelll\n",
      "epoch: 67, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 68, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 69, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 70, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 71, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 72, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 73, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 74, loss: 0.526\n",
      "Predicted string:  ihelll\n",
      "epoch: 75, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 76, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 77, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 78, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 79, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 80, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 81, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 82, loss: 0.525\n",
      "Predicted string:  ihelll\n",
      "epoch: 83, loss: 0.524\n",
      "Predicted string:  ihelll\n",
      "epoch: 84, loss: 0.524\n",
      "Predicted string:  ihelll\n",
      "epoch: 85, loss: 0.524\n",
      "Predicted string:  ihelll\n",
      "epoch: 86, loss: 0.524\n",
      "Predicted string:  ihelll\n",
      "epoch: 87, loss: 0.524\n",
      "Predicted string:  ihelll\n",
      "epoch: 88, loss: 0.524\n",
      "Predicted string:  ihelll\n",
      "epoch: 89, loss: 0.524\n",
      "Predicted string:  ihelll\n",
      "epoch: 90, loss: 0.523\n",
      "Predicted string:  ihello\n",
      "epoch: 91, loss: 0.523\n",
      "Predicted string:  ihello\n",
      "epoch: 92, loss: 0.523\n",
      "Predicted string:  ihello\n",
      "epoch: 93, loss: 0.522\n",
      "Predicted string:  ihello\n",
      "epoch: 94, loss: 0.522\n",
      "Predicted string:  ihello\n",
      "epoch: 95, loss: 0.521\n",
      "Predicted string:  ihello\n",
      "epoch: 96, loss: 0.520\n",
      "Predicted string:  ihello\n",
      "epoch: 97, loss: 0.518\n",
      "Predicted string:  ihello\n",
      "epoch: 98, loss: 0.514\n",
      "Predicted string:  ihello\n",
      "epoch: 99, loss: 0.510\n",
      "Predicted string:  ihello\n",
      "epoch: 100, loss: 0.513\n",
      "Predicted string:  ihello\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = rnn(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RNN with Embeddings\n",
    "\n",
    "![image.png](img/dl52.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T15:53:19.862473Z",
     "start_time": "2021-08-08T15:53:19.856431Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5150, -0.6074,  2.5425, -0.0226,  0.2476]],\n",
       "\n",
       "        [[ 1.2734, -0.1155,  1.2753,  2.0634, -1.2319]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# nn.Embedding is a look-up table \n",
    "# As no model has been trained, they will be random.\n",
    "embedding = nn.Embedding(10, 5)\n",
    "# it contains 10 tensors (vocab_size = 10) of size 5 (embedding_dim = 3)\n",
    "# a batch of 2 samples of 1 indices each \n",
    "input = torch.LongTensor([[1],[4]])\n",
    "embedding(input)\n",
    "# return the embedding vectors corresponding \n",
    "# to the word 1 and 4 in your vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T14:45:43.634856Z",
     "start_time": "2021-08-08T14:45:43.625524Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lab 12 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.LongTensor(x_data))\n",
    "labels = Variable(torch.LongTensor(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T14:45:47.211100Z",
     "start_time": "2021-08-08T14:45:47.207674Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "input_size = 5\n",
    "# Note: add embedding size\n",
    "embedding_size = 10  # embedding size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence \n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T14:45:47.838996Z",
     "start_time": "2021-08-08T14:45:47.833208Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) \n",
    "        # input_size: 有几个不同的词汇；embedding_size: 把这些词汇embed到几个维度的空间里\n",
    "        self.rnn = nn.RNN(input_size=embedding_size,\n",
    "                          hidden_size=5, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h_0 = Variable(torch.zeros(num_layers, x.size(0), hidden_size))\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(batch_size, sequence_length, -1)\n",
    "        # Propagate embedding through RNN\n",
    "        # Input: (batch, seq_len, embedding_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)        \n",
    "        out, _ = self.rnn(emb, h_0)\n",
    "        return self.fc(out.view(-1, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T14:45:48.702559Z",
     "start_time": "2021-08-08T14:45:48.695446Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(5, 10)\n",
      "  (rnn): RNN(10, 5, batch_first=True)\n",
      "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RNN model\n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-08T14:45:50.112980Z",
     "start_time": "2021-08-08T14:45:49.997065Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 1.768\n",
      "Predicted string:  eheohh\n",
      "epoch: 2, loss: 1.396\n",
      "Predicted string:  oheiii\n",
      "epoch: 3, loss: 1.132\n",
      "Predicted string:  iheloo\n",
      "epoch: 4, loss: 0.949\n",
      "Predicted string:  ihello\n",
      "epoch: 5, loss: 0.798\n",
      "Predicted string:  ihelll\n",
      "epoch: 6, loss: 0.659\n",
      "Predicted string:  ihelll\n",
      "epoch: 7, loss: 0.547\n",
      "Predicted string:  ihelll\n",
      "epoch: 8, loss: 0.449\n",
      "Predicted string:  ihelll\n",
      "epoch: 9, loss: 0.384\n",
      "Predicted string:  ihelll\n",
      "epoch: 10, loss: 0.343\n",
      "Predicted string:  ihello\n",
      "epoch: 11, loss: 0.304\n",
      "Predicted string:  ihello\n",
      "epoch: 12, loss: 0.235\n",
      "Predicted string:  ihello\n",
      "epoch: 13, loss: 0.227\n",
      "Predicted string:  ihello\n",
      "epoch: 14, loss: 0.239\n",
      "Predicted string:  iheloo\n",
      "epoch: 15, loss: 0.233\n",
      "Predicted string:  iheloo\n",
      "epoch: 16, loss: 0.146\n",
      "Predicted string:  ihello\n",
      "epoch: 17, loss: 0.320\n",
      "Predicted string:  ihelll\n",
      "epoch: 18, loss: 0.139\n",
      "Predicted string:  ihello\n",
      "epoch: 19, loss: 0.228\n",
      "Predicted string:  iheloo\n",
      "epoch: 20, loss: 0.242\n",
      "Predicted string:  iheloo\n",
      "epoch: 21, loss: 0.191\n",
      "Predicted string:  iheloo\n",
      "epoch: 22, loss: 0.115\n",
      "Predicted string:  ihello\n",
      "epoch: 23, loss: 0.157\n",
      "Predicted string:  ihelll\n",
      "epoch: 24, loss: 0.133\n",
      "Predicted string:  ihello\n",
      "epoch: 25, loss: 0.105\n",
      "Predicted string:  ihello\n",
      "epoch: 26, loss: 0.127\n",
      "Predicted string:  ihello\n",
      "epoch: 27, loss: 0.135\n",
      "Predicted string:  ihello\n",
      "epoch: 28, loss: 0.109\n",
      "Predicted string:  ihello\n",
      "epoch: 29, loss: 0.078\n",
      "Predicted string:  ihello\n",
      "epoch: 30, loss: 0.099\n",
      "Predicted string:  ihello\n",
      "epoch: 31, loss: 0.074\n",
      "Predicted string:  ihello\n",
      "epoch: 32, loss: 0.061\n",
      "Predicted string:  ihello\n",
      "epoch: 33, loss: 0.074\n",
      "Predicted string:  ihello\n",
      "epoch: 34, loss: 0.064\n",
      "Predicted string:  ihello\n",
      "epoch: 35, loss: 0.045\n",
      "Predicted string:  ihello\n",
      "epoch: 36, loss: 0.062\n",
      "Predicted string:  ihello\n",
      "epoch: 37, loss: 0.040\n",
      "Predicted string:  ihello\n",
      "epoch: 38, loss: 0.044\n",
      "Predicted string:  ihello\n",
      "epoch: 39, loss: 0.047\n",
      "Predicted string:  ihello\n",
      "epoch: 40, loss: 0.034\n",
      "Predicted string:  ihello\n",
      "epoch: 41, loss: 0.038\n",
      "Predicted string:  ihello\n",
      "epoch: 42, loss: 0.035\n",
      "Predicted string:  ihello\n",
      "epoch: 43, loss: 0.030\n",
      "Predicted string:  ihello\n",
      "epoch: 44, loss: 0.034\n",
      "Predicted string:  ihello\n",
      "epoch: 45, loss: 0.030\n",
      "Predicted string:  ihello\n",
      "epoch: 46, loss: 0.026\n",
      "Predicted string:  ihello\n",
      "epoch: 47, loss: 0.031\n",
      "Predicted string:  ihello\n",
      "epoch: 48, loss: 0.024\n",
      "Predicted string:  ihello\n",
      "epoch: 49, loss: 0.025\n",
      "Predicted string:  ihello\n",
      "epoch: 50, loss: 0.025\n",
      "Predicted string:  ihello\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(50):\n",
    "    outputs = model(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img src=\"img/dl64.png\" align = 'right' width = \"500\"></div>\n",
    "\n",
    "**Practical PyTorch: Classifying Names with a Character-Level RNN**\n",
    "\n",
    "We will train a basic character-level RNN to classify words. It reads words as a series of characters - outputting a prediction and \"hidden state\" at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to. \n",
    "\n",
    "Specifically, we'll train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling.\n",
    "\n",
    "https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Sentiment analysis on movie reviews\n",
    "\n",
    "The sentiment labels are:\n",
    "\n",
    "```\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive\n",
    "```\n",
    "\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/chengjun2.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

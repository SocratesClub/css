{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "# Attention Mechanisms and Transformers\n",
    "---\n",
    "---\n",
    "\n",
    "![image.png](img/chengjun.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under the hood: RNN\n",
    "\n",
    "![image.png](img/dl53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl54.png)\n",
    "\n",
    "https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Under the hood: LSTMs \n",
    "\n",
    "Long Short Term Memory networks \n",
    "\n",
    "![image.png](img/dl56.png)\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl60.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl61.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under the hood: GRU\n",
    "\n",
    "![image.png](img/dl62.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl63.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html\n",
    "    \n",
    "![image-2.png](./img/attention1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention Mechanisms: Self-Attention\n",
    "\n",
    "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf\n",
    "\n",
    "”They welcomed the **queen** of the united kindom.“ \n",
    "\n",
    "    \n",
    "![image-2.png](./img/attention2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms: Multi-head Self-Attention\n",
    "\n",
    "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf  \n",
    "\n",
    "They **played** chess\n",
    "\n",
    "动词vs.过去时态被分别表征\n",
    "\n",
    "    \n",
    "![image-2.png](./img/attention0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms: Multi-head Self-Attention\n",
    "\n",
    "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf  \n",
    "\n",
    "They **played** chess: 动词vs.过去时态被分别表征\n",
    "\n",
    "    \n",
    "![image-2.png](./img/attention3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms and Transformers\n",
    "\n",
    "http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html\n",
    "    \n",
    "    \n",
    "![image-2.png](./img/attention4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms and Bert\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. \n",
    "\n",
    "![image-2.png](./img/attention5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms and Bert\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "The embeddings of the BERT input sequence are the sum of the token embeddings, segment embeddings, and positional embeddings.\n",
    "\n",
    "![image-2.png](./img/attention6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms, Transformers, and GPT\n",
    "\n",
    "In 2018, OpenAI published a paper (Improving Language Understanding by Generative Pre-Training) about using natural language understanding using their GPT-1 language model. This model was a proof-of-concept and was not released publicly.\n",
    "\n",
    "![image-2.png](./img/attention7.png)\n",
    "\n",
    "https://www.datacamp.com/blog/what-we-know-gpt4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/chengjun2.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

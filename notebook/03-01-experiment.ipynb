{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Running Experiment\n",
    "\n",
    "\n",
    "## Bit by Bit: Social Research in the Digital Age\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "![image.png](img/chengjun.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "https://www.bitbybitbook.com/en/1st-ed/running-experiments/\n",
    "\n",
    "![image.png](img/bit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.1 Introduction\n",
    "\n",
    "When researchers run experiments, they **systematically intervene in the world** to create data that is ideally suited to answering questions about **cause-and-effect** relationships.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T04:20:43.939012Z",
     "start_time": "2020-05-07T04:20:43.921192Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" \n",
       "    width=400 height=400 \n",
       "    src=\"//music.163.com/outchain/player?type=1&id=1678852&auto=1&height=200\">\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" \n",
    "    width=400 height=400 \n",
    "    src=\"//music.163.com/outchain/player?type=1&id=1678852&auto=1&height=200\">\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Experiments enable researchers to **move beyond the correlations** in naturally occurring data in order to reliably answer certain cause-and-effect questions. \n",
    "\n",
    "- In the analog age, experiments were often logistically difficult and expensive. \n",
    "- Now, in the digital age, logistical constraints are gradually fading away. \n",
    "    - Not only is it easier to do experiments like those done in the past, \n",
    "    - it is now possible to run new kinds of experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Experiments \n",
    "    - Perturb-and-observe experiments, \n",
    "    - involve only a single group that has received the intervention\n",
    "- **Randomized Controlled Experiments**\n",
    "    - intervenes for some people and not for others\n",
    "    - the researcher decides which people receive the intervention by randomization\n",
    "        - create fair comparisons between two groups: one that has received the intervention and one that has not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.2 What are experiments?\n",
    "Randomized controlled experiments have four main ingredients: \n",
    "- recruitment of participants, \n",
    "- randomization of treatment, \n",
    "- delivery of treatment, \n",
    "- measurement of outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The digital age does not change the fundamental nature of experimentation, but it does make it easier logistically. \n",
    "\n",
    "For example, in the past, it might have been difficult to measure the behavior of millions of people, but that is now routinely happening in many digital systems. Researchers who can figure out how to harness these new opportunities will be able to run experiments that were impossible previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**An experiment by Michael Restivo and Arnout van de Rijt (2012)**\n",
    "\n",
    "They wanted to understand the effect of informal peer rewards on editorial contributions to Wikipedia. \n",
    "- they gave **barnstars** to 100 deserving Wikipedians.\n",
    "- they also picked 100 top contributors as the control group to whom they did not give barnstars.\n",
    "- the treatment group and control group was determined randomly.\n",
    "- they tracked the recipients’ subsequent contributions. \n",
    "\n",
    "The recipients tended to make fewer edits after receiving one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When Restivo and van de Rijt looked at the behavior of people in the control group, they found that their contributions were decreasing too. \n",
    "\n",
    "Further, they compared people in the treatment group (i.e., received barnstars) to people in the control group, they found that people in the treatment group contributed about 60% more. \n",
    "\n",
    "In other words, the contributions of both groups were deceasing, but those of the control group were doing so much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The logistics of digital experiments can be completely different from those of analog experiments. In Restivo and van de Rijt’s experiment, \n",
    "- it was easy to give the barnstar to anyone, \n",
    "- it was easy to track the outcome—number of edits—over an extended period of time \n",
    "    - because edit history is automatically recorded by Wikipedia.\n",
    "\n",
    "**Why don't they scale up their experiment to millions of people?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.3 Two dimensions of experiments: lab-field and analog-digital\n",
    "Lab experiments offer control, field experiments offer realism, and digital field experiments combine control and realism at scale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex2.png)\n",
    "Schematic of design space for experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Field experiments combine the strong design of randomized control experiments with more representative groups of participants performing more common tasks in more natural settings.\n",
    "\n",
    "Lab and field experiments are complementary, with different strengths and weaknesses. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, Correll, Benard, and Paik (2007) used both a lab experiment and a field experiment in an attempt to find the sources of the “motherhood penalty.” \n",
    "\n",
    "- Mothers earn less money than childless women, even when comparing women with similar skills working in similar jobs. \n",
    "- Interestingly, the opposite seems to be true for fathers: they tend to earn more than comparable childless men.\n",
    "- One explanation is that employers are biased against mothers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, in the lab experiment \n",
    "- they told college undergraduates that a company was conducting an employment search for a person to lead its new East Coast marketing department. \n",
    "- The students were told that the company wanted their help in the hiring process\n",
    "- they were asked to review resumes of several potential candidates and to rate the candidates on a number of dimensions, such as their intelligence, warmth, and commitment to work. \n",
    "- Further, the students were asked if they would recommend hiring the applicant and what they would recommend as a starting salary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The lab experiment allowed Correll and colleagues to measure a causal effect and provide a possible explanation for that effect.\n",
    "- Correll and colleagues found that the students were less likely to recommend hiring the mothers and that they offered them a lower starting salary. \n",
    "- Further, through a statistical analysis of both the ratings and the hiring-related decisions, Correll and colleagues found that mothers’ disadvantages were largely explained by the fact that they were rated lower in terms of competence and commitment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Correll and colleagues also conducted a complementary field experiment. \n",
    "- They responded to hundreds of advertised job openings with fake cover letters and resumes. Some resumes signaled motherhood and some did not. \n",
    "- Correll and colleagues found that mothers were less likely to get called back for interviews than equally qualified childless women. \n",
    "\n",
    "In other words, real employers making consequential decisions in a natural setting behaved much like the undergraduates. Did they make similar decisions for the same reason? Unfortunately, we don’t know. The researchers were not able to ask the employers to rate the candidates or explain their decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Researchers who prefer Lab experiments argue that \n",
    "\n",
    "- Lab experiments offer researchers near-total control of the environment in which participants are making decisions. \n",
    "- Lab experiments can collect additional data that can help explain why participants are making their decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Researchers who prefer field experiments argue that \n",
    "- participants in lab experiments could act very differently because they know that they are being studied.\n",
    "- the lab experiment will overestimate the effect of motherhood on real hiring decisions. \n",
    "- lab experiments’ reliance on WEIRD participants: mainly students from Western, Educated, Industrialized, Rich, and Democratic countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As digital devices become increasingly integrated into people’s lives and sensors become integrated into the built environment, these opportunities to run partially digital experiments in the physical world will increase dramatically. \n",
    "\n",
    "> Digital experiments are not just online experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The digital age also creates the possibility of running lab-like experiments online. For example, researchers have rapidly adopted Amazon Mechanical Turk (MTurk) to recruit participants for online experiments \n",
    "\n",
    "![image.png](img/ex3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, whereas most analog lab and field experiments have hundreds of participants, digital field experiments can have millions of participants.\n",
    "\n",
    "Second, whereas most analog lab and field experiments treat participants as indistinguishable widgets, digital field experiments often use background information about participants in the design and analysis stages of the research. \n",
    "\n",
    "Third, whereas many analog lab and field experiments deliver treatments and measure outcomes in a relatively compressed amount of time, some digital field experiments happen over much longer timescales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While digital field experiments offer many possibilities, they also share some weaknesses with both analog lab and analog field experiments. \n",
    "- experiments cannot be used to study the past, and they can only estimate the effects of treatments that can be manipulated. \n",
    "- although experiments are undoubtedly useful to guide policy, the exact guidance they can offer is somewhat limited because of complications such as environmental dependence, compliance problems, and equilibrium effects (Banerjee and Duflo 2009; Deaton 2010). \n",
    "- Digital field experiments also magnify the ethical concerns created by field experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.4 Moving beyond simple experiments\n",
    "\n",
    "- Validity \n",
    "- Heterogeneity of treatment effects\n",
    "- Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Simple Experiments**\n",
    "\n",
    "Simple experiment narrowly focused on a much more specific question: \n",
    "> What is the average effect of this specific treatment with this specific implementation for this population of participants at this time? \n",
    "\n",
    "Unfortunately, loose phrasing about what “works” obscures the fact that narrowly focused experiments don’t really tell you whether a treatment “works” in a general sense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Simple experiments can provide valuable information, but they fail to answer many questions that are both important and interesting, such as \n",
    "\n",
    "> whether there are some people for whom the treatment had a larger or smaller effect; \n",
    "\n",
    "> whether there is another treatment that would be more effective; \n",
    "\n",
    "> whether this experiment relates to broader social theories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Field experiment by P. Wesley Schultz and colleagues on the relationship between social norms and energy consumption (Schultz et al. 2007).\n",
    "\n",
    "![image.png](img/ex4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fortunately, Schultz and colleagues did not settle for this simplistic analysis. Before the experiment began, they reasoned that heavy users of electricity—people above the mean—might reduce their consumption, and that light users of electricity—people below the mean—might actually increase their consumption. \n",
    "\n",
    "![image.png](img/ex5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With-in subjects design\n",
    "![image.png](img/ex6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.1 Validity\n",
    "\n",
    "Validity refers to the extent to which the results of a particular experiment support some more general conclusion. \n",
    "\n",
    "Social scientists split validity into four main types: \n",
    "- statistical conclusion validity, \n",
    "- internal validity, \n",
    "- construct validity, \n",
    "- external validity \n",
    "\n",
    "(Shadish, Cook, and Campbell 2001, chap. 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Statistical conclusion validity centers around whether the statistical analysis of the experiment was done correctly. \n",
    "- Internal validity centers around whether the experimental procedures were performed correctly.\n",
    "- Construct validity centers around the match between the data and the theoretical constructs.\n",
    "- External validity centers around whether the results of this experiment can be generalized to other situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The four types of validity provide a mental checklist to help researchers assess whether the results from a particular experiment support a more general conclusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A company named Opower partnered with utilities in the United States to deploy the treatment more widely, inspired by Schultz et al. (2007). \n",
    "- In a first set of experiments involving 600,000 households from 10 different sites, Allcott (2011) found that the Home Energy Report lowered electricity consumption. \n",
    "- Further, in subsequent research (Allcott 2015) involving 8 million additional households from 101 different sites. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex7.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex8.png)\n",
    "\n",
    "Action Step Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex9.png)\n",
    "\n",
    "**The size of the effect declined in the later experiments.** \n",
    "\n",
    "Allcott (2015) argues that a major source of this pattern is that\n",
    "\n",
    "> sites with more environmentally-focused customers were more likely to adopt the program earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.2 Heterogeneity of treatment effects\n",
    "\n",
    "Experiments normally measure the average effect, but the effect is probably not the same for everyone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Costa and Kahn (2013) speculated that the effectiveness of the Home Energy Report could vary based on a participant’s political ideology. They merged the Opower data with data purchased from a third-party aggregator. \n",
    "![image.png](img/ex10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.4.3 Mechanisms\n",
    "- Patterns (captured by experiments) measure what happened. \n",
    "- Mechanisms explain why and how it happened.\n",
    "\n",
    "Although experiments are good for estimating causal effects, they are often not designed to reveal mechanisms. Digital experiments can help us identify mechanisms in two ways: \n",
    "- (1) they enable us to collect more process data \n",
    "- (2) they enable us to test many related treatments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One way to test possible mechanisms is by <u>collecting process data</u> about how the treatment impacted possible mechanisms.\n",
    "\n",
    "In a follow-up study, Allcott and Rogers (2014) partnered with a power company that, through a rebate program, had acquired information about which consumers upgraded their appliances to more energy-efficient models. \n",
    "\n",
    "- slightly more people receiving the Home Energy Reports upgraded their appliances. \n",
    "- But this difference was so small that it could account for only 2% of the decrease in energy use in the treated households. \n",
    "\n",
    "In other words, appliance upgrades were not the dominant mechanism through which the Home Energy Report decreased electricity consumption.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A second way to study mechanisms is to run experiments with slightly different versions of the treatment that <u>enables full factorial designs</u>.\n",
    "- tips vs. peer effect\n",
    "    - To assess the possibility that the tips alone might have been sufficient\n",
    "        - Ferraro, Miranda, and Price (2011)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A full factorial design\n",
    "\n",
    "| Treatment | Characteristics                  |\n",
    "| :-------- | :------------------------------- |\n",
    "| 1         | Control                          |\n",
    "| 2         | Tips                             |\n",
    "| 3         | Appeal                           |\n",
    "| 4         | Peer Information                 |\n",
    "| 5         | Tips + appeal                    |\n",
    "| 6         | Tips + peer information          |\n",
    "| 7         | Appeal + peer information        |\n",
    "| 8         | Tips + appeal + peer information |\n",
    "\n",
    "\n",
    "The digital age can enable full factorial designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.5 Making it happen\n",
    "- Even if you don’t work at a big tech company you can run digital experiments. \n",
    "- You can either do it yourself or partner with someone who can help you (and who you can help)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.5.1 Use existing environments\n",
    "You can run experiments inside existing environments, often without any coding or partnership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doleac and Stein’s iPod advertisements varied along three main dimensions:\n",
    "\n",
    "- characteristics of the seller, which was signaled by the hand photographed holding the iPod [white, black, white with tattoo] \n",
    "- the asking price [90, 110, 130]. \n",
    "- the quality of the ad text [high-quality and low-quality (e.g., capitalization errors and speling errors)]. \n",
    "\n",
    "Thus, the authors had a 3 × 3 × 2 design which was deployed across more than 300 local markets, ranging from towns to mega-cities.\n",
    "\n",
    "    Doleac, Jennifer L., and Luke C.D. Stein. 2013. “The Visible Hand: Race and Online Market Outcomes.” Economic Journal 123 (572):F469–F492. https://doi.org/10.1111/ecoj.12082."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Averaged across all conditions, the outcomes were better for the white sellers than the black sellers, with the tattooed sellers having intermediate results.\n",
    "![image.png](img/ex14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Arnout van de Rijt and colleagues (2014): The keys to success**\n",
    "\n",
    "    Puzzlement: In many aspects of life, seemingly similar people end up with very different outcomes. \n",
    "\n",
    "One possible explanation for this pattern is that small—and essentially random—advantages can lock in and grow over time, a process that researchers call **cumulative advantage**. \n",
    "\n",
    "\n",
    "Rijt, Arnout van de, Soong Moon Kang, Michael Restivo, and Akshay Patil. 2014. “Field Experiments of Success-Breeds-Success Dynamics.” PNAS. 111 (19):6934–9. https://doi.org/10.1073/pnas.1316836111."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to determine whether small initial successes lock in or fade away\n",
    "- van de Rijt and colleagues (2014) intervened in four different systems bestowing success on randomly selected participants, and then measured the subsequent impacts of this arbitrary success.\n",
    "\n",
    "![image.png](img/ex15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Topic                                                        | References                                                   |\n",
    "| :----------------------------------------------------------- | :----------------------------------------------------------- |\n",
    "| Effect of barnstars on contributions to Wikipedia            | Restivo and Rijt (2012); Restivo and Rijt (2014); Rijt et al. (2014) |\n",
    "| Effect of anti-harassment message on racist tweets           | Munger (2016)                                                |\n",
    "| Effect of auction method on sale price                       | Lucking-Reiley (1999)                                        |\n",
    "| Effect of reputation on price in online auctions             | Resnick et al. (2006)                                        |\n",
    "| Effect of race of seller on sale of baseball cards on eBay   | Ayres, Banaji, and Jolls (2015)                              |\n",
    "| Effect of race of seller on sale of iPods                    | Doleac and Stein (2013)                                      |\n",
    "| Effect of race of guest on Airbnb rentals                    | Edelman, Luca, and Svirsky (2016)                            |\n",
    "| Effect of donations on the success of projects on Kickstarter | Rijt et al. (2014)                                           |\n",
    "| Effect of race and ethnicity on housing rentals              | Hogan and Berry (2011)                                       |\n",
    "| Effect of positive rating on future ratings on Epinions      | Rijt et al. (2014)                                           |\n",
    "| Effect of signatures on the success of petitions             | Vaillant et al. (2015); Rijt et al. (2014); Rijt et al. (2016) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.5.2 Build your own experiment\n",
    "\n",
    "Building your own experiment might be costly, but it will enable you to create the experiment that you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gregory Huber et al. (2012) explored voters' three biases\n",
    "\n",
    "- (1) they are focused on recent rather than cumulative performance; \n",
    "- (2) they can be manipulated by rhetoric, framing, and marketing;\n",
    "- (3) they can be influenced by events unrelated to incumbent performance, such as the success of local sports teams and the weather. \n",
    "\n",
    "It was hard to isolate any of these factors. Therefore, they created a highly simplified voting environment in order to isolate, and then experimentally study, each of these three possible biases.\n",
    "\n",
    "> Huber, Gregory A., Seth J. Hill, and Gabriel S. Lenz. 2012. Sources of Bias in Retrospective Decision Making: Experimental Evidence on Voters Limitations in Controlling Incumbents. American Political Science Review 106 (4):720–41."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Huber and colleagues used MTurk to recruit participants. \n",
    "- Once a participant provided informed consent and passed a short test, she was told that she was participating in a 32-round game to earn tokens that could be converted into real money. \n",
    "- At the beginning of the game, each participant was told that she had been assigned an “allocator” that would give her free tokens each round and that some allocators were more generous than others. \n",
    "- Further, each participant was also told that she would have a chance to either keep her allocator or be assigned a new one after 16 rounds of the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In total, Huber and colleagues recruited about 4,000 participants who were paid about $1.25 for a task that took about eight minutes.\n",
    "\n",
    "To assess whether participants voting decisions could be influenced by purely random events in their setting, Huber and colleagues added a lottery to their experimental system. At either the 8th round or the 16th round."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Centola (2010) built a digital field experiment to study the effect of social network structure on the spread of behavior. \n",
    "\n",
    "Centola built a web-based health community. Centola recruited about 1,500 participants through advertising on health websites. When participants arrived at the online community—which was called the Healthy Lifestyle Network—they provided informed consent and were then assigned “health buddies.” Because of the way Centola assigned these health buddies, he was able to knit together different social network structures in different groups. \n",
    "\n",
    "Centola, D. 2010. “The Spread of Behavior in an Online Social Network Experiment.” Science 329 (5996):1194–7."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex17.png)\n",
    "\n",
    "A popular hypothesis states that networks with many clustered ties and a high degree of separation will be less effective for behavioral diffusion than networks in which locally redundant ties are rewired to provide shortcuts across the social space. A competing hypothesis argues that when behaviors require social reinforcement, a network with more clustering may be more advantageous, even if the network as a whole has a larger diameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, Centola introduced a new behavior into each network: the chance to register for a new website with additional health information. Whenever anyone signed up for this new website, all of her health buddies received an email announcing this behavior. \n",
    "\n",
    "Centola found that this behavior—signing up for the new website—spread further and faster in the clustered network than in the random network, a finding that was contrary to some existing theories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Overall, building your own experiment gives you much more control; it enables you to construct the best possible environment to isolate what you want to study. Further, building your own system decreases ethical concerns around experimenting in existing systems. When you build your own experiment, however, you run into many of the problems that are encountered in lab experiments: recruiting participants and concerns about realism. A final downside is that building your own experiment can be costly and time-consuming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.5.3 Build your own product\n",
    "Building your own product is a high-risk, high-reward approach. But, if it works, you can benefit from a positive feedback loop that enables distinctive research\n",
    "\n",
    "![image.png](img/ex18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.5.4 Partner with the powerful\n",
    "\n",
    "Partnering can reduce costs and increase scale, but it can alter the kinds of participants, treatments, and outcomes that you can use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While working on a commercial fermentation project to convert beet juice into alcohol, Pasteur discovered a new class of microorganism that eventually led to the germ theory of disease.\n",
    "\n",
    "![image.png](img/ex19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On November 2, 2010—the day of the US congressional elections—all 61 million Facebook users who lived in the United States and were 18 and older took part in an experiment about voting.\n",
    "\n",
    "Bond, Robert M., Christopher J. Fariss, Jason J. Jones, Adam D. I. Kramer, Cameron Marlow, Jaime E. Settle, and James H. Fowler. 2012. “A 61-Million-Person Experiment in Social Influence and Political Mobilization.” Nature 489 (7415):295–98.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Topic                                                        | References                                                   |\n",
    "| :----------------------------------------------------------- | :----------------------------------------------------------- |\n",
    "| Effect of Facebook News Feed on information sharing          | Bakshy, Rosenn, et al. (2012)                                |\n",
    "| Effect of partial anonymity on behavior on online dating website | Bapna et al. (2016)                                          |\n",
    "| Effect of Home Energy Reports on electricity usage           | Allcott (2011); Allcott and Rogers (2014); Allcott (2015); Costa and Kahn (2013); Ayres, Raseman, and Shih (2013) |\n",
    "| Effect of app design on viral spread                         | Aral and Walker (2011)                                       |\n",
    "| Effect of spreading mechanism on diffusion                   | S. J. Taylor, Bakshy, and Aral (2013)                        |\n",
    "| Effect of social information in advertisements               | Bakshy, Eckles, et al. (2012)                                |\n",
    "| Effect of catalog frequency on sales through catalog and online for different types of customers | Simester et al. (2009)                                       |\n",
    "| Effect of popularity information on potential job applications | Gee (2015)                                                   |\n",
    "| Effect of initial ratings on popularity                      | Muchnik, Aral, and Taylor (2013)                             |\n",
    "| Effect of message content on political mobilization          | Coppock, Guess, and Ternovski (2016)                         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4.6 Advices\n",
    "- when you are doing an experiment is that you should think as much as possible before any data has been collected. \n",
    "- no single experiment is going to be perfect, and you should consider designing a series of experiments that reinforce each other.\n",
    "\n",
    "Two pieces of advice that are more specific for designing digital age experiments: create zero variable cost data and build ethics into your design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.6.1 Create zero variable cost data\n",
    "The key to running large experiments is to drive your variable cost to zero. The best ways to do this are automation and designing enjoyable experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to create experiments with zero variable cost data, you’ll need to ensure that everything is fully automated and that participants don’t require any payment.\n",
    "\n",
    "| Compensation                    | References                                                   |\n",
    "| :------------------------------ | :----------------------------------------------------------- |\n",
    "| Website with health information | Centola (2010)                                               |\n",
    "| Exercise program                | Centola (2011)                                               |\n",
    "| Free music                      | Salganik, Dodds, and Watts (2006); Salganik and Watts (2008); Salganik and Watts (2009b) |\n",
    "| Fun game                        | Kohli et al. (2012)                                          |\n",
    "| Movie recommendations           | Harper and Konstan (2015)                 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex20.png)\n",
    "\n",
    "MusicLab was able to run at essentially zero variable cost because of the way that it was designed. First, everything was fully automated so it was able to run while I was sleeping. Second, the compensation was free music, so there was no variable participant compensation cost. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Musiclab Data Release** \n",
    "\n",
    "https://opr.princeton.edu/archive/cm/\n",
    "\n",
    "Using a \"multiple worlds\" experimental design, four experiments involving a total of 27,267 participants. Included in this release are 167 data files containing the experimental results, mp3 files from the 48 songs, and the data documentation. The data files are in the ascii text, comma separated values(csv) format.\n",
    "\n",
    "These data files are to reproduce, and hopefully expand upon, the analysis conducted in the dissertation project by Matthew J. Salganik, supervised by Duncan J. Watts. The experiments were conducted at the Department of Sociology at Columbia University between 2004 and 2007.\n",
    "\n",
    "Please direct any questions to Prof. Matthew Salganik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.6.2 Build ethics into your design: replace, refine, and reduce\n",
    "Make your experiment more humane by replacing experiments with non-experimental studies, refining the treatments, and reducing the number of participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Kramer, Guillory, and Hancock (2014)** Facebook Emotion Contagion\n",
    "\n",
    "Facebook News Feed, an algorithmically curated set of Facebook status updates from a user’s Facebook friends. \n",
    "- H0: because the News Feed has mostly positive posts—friends showing off their latest party—it could cause users to feel sad because their lives seemed less exciting in comparison. \n",
    "- H1: seeing your friend having a good time would make you feel happy. \n",
    "\n",
    "In order to address these competing hypotheses—and to advance our understanding of how a person’s emotions are impacted by her friends’ emotions—Kramer and colleagues ran an experiment. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "They placed about 700,000 users into four groups for one week: \n",
    "- a “negativity-reduced” group, for whom posts with negative words (e.g., “sad”) were randomly **blocked** from appearing in the News Feed; \n",
    "- a “positivity-reduced” group for whom posts with positive words (e.g., “happy”) were randomly **blocked**; \n",
    "- and two control groups. \n",
    "\n",
    "Kramer, Adam D. I., Jamie E. Guillory, and Jeffrey T. Hancock. 2014. “Experimental Evidence of Massive-Scale Emotional Contagion Through Social Networks.” Proceedings of the National Academy of Sciences of the USA 111 (24):8788–90."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Construct Validity\n",
    "    - counts of words\n",
    "- There is no analysis on heterogeneity of treatment effects the mechanisms\n",
    "- The effect size in this experiment was very small\n",
    "    - 1/1000 words\n",
    "- An enormous outcry from both researchers and the press\n",
    "    - manipulations of information exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The first R is replace**: researchers should seek to replace experiments with less invasive and risky techniques, if possible. \n",
    "\n",
    "Lorenzo Coviello et al. (2014) used random variation in the weather (instrumental variable) to study the effect of changes in the Facebook News Feed without the need to intervene at all.\n",
    "\n",
    "\n",
    "> Coviello, Lorenzo, Yunkyu Sohn, Adam D. I. Kramer, Cameron Marlow, Massimo Franceschetti, Nicholas A. Christakis, and James H. Fowler. 2014. “Detecting Emotional Contagion in Massive Social Networks.” PLoS ONE 9 (3):e90315."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Prior to estimation we use an instrument $X_{gt}$, the aggregated rainfall of the friends of the people in subpopulation $g$, to predict exogenous variation in the friends' emotional expression $Y_{gt}$:\n",
    "\n",
    "\n",
    "$$Y_{gt} = \\theta_{t}^{'} + c_g^{'} + \\beta_1 X_{gt} + \\beta_2 x_{gt} + \\epsilon_{gt}^{'}  \\space \\space \\space \\space (1)$$\n",
    "\n",
    "Using the fittled value of $\\hat Y_{gt}$, we fit the 2SLS regression:\n",
    "\n",
    "$$y_{gt} = \\theta_t + c_g + \\beta x_{gt} + \\gamma \\hat Y_{gt} + \\epsilon_{gt}  \\space \\space \\space \\space (2)$$\n",
    "\n",
    "where for time $t$, $y_{gt}$ is the average emotion of all people in subpopulation (city) $g$; $θ_t$ and $c_g$ are time and subpopulation fixed effects; $x_{gt}$ is the average exogenous factor (rainfall) for people in subpopulation $g$; $Y_{gt}$ is a weighted average emotional expression of friends of people in subpopulation $g$; and $ε_{gt}$ is an error term.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/ex23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second of the three Rs is refine:researchers should seek to refine their treatments to make them as harmless as possible. \n",
    "\n",
    "- For example, rather than blocking content that was either positive or negative, the researchers could have boosted content that was positive or negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The third R is reduce: researchers should seek to reduce the number of participants in their experiment to the minimum needed to achieve their scientific objective. \n",
    "\n",
    "- An approach that is sometimes called a mixed design and sometimes called a difference-in-differences estimator. \n",
    "\n",
    "    For each participant, the researchers could:\n",
    "    - create a change score (post-treatment behavior − pre-treatment behavior) \n",
    "    - compare the change scores of participants in the treatment and control conditions. \n",
    "\n",
    "This difference-in-differences approach is more efficient statistically, which means that researchers can achieve the same statistical confidence using much smaller samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/chengjun2.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

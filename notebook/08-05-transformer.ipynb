{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "# Attention Mechanisms and Transformers\n",
    "---\n",
    "---\n",
    "\n",
    "![image.png](img/chengjun.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under the hood: RNN\n",
    "\n",
    "![image.png](img/dl53.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl54.png)\n",
    "\n",
    "https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Under the hood: LSTMs \n",
    "\n",
    "Long Short Term Memory networks \n",
    "\n",
    "![image.png](img/dl56.png)\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl60.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl61.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under the hood: GRU\n",
    "\n",
    "![image.png](img/dl62.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image.png](img/dl63.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html\n",
    "    \n",
    "![image-2.png](./img/attention1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html\n",
    "\n",
    "![image-2.png](./img/attention11.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "![image-2.png](./img/attention12.png)\n",
    "\n",
    "W ᑫ, W ᵏ, W ᵛ is the target to train in a layer. Attention is combinations of Query and Key, where Query act as giver and Key act as receiver. Value can seem as information extractors, will extract a unique value based on attentions of words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Where are W ᑫ, W ᵏ, W ᵛ matrices coming from in Attention model?\n",
    "\n",
    "W ᑫ, W ᵏ, W ᵛ is the target to train in a layer. \n",
    "- Attention is combinations of Query and Key\n",
    "- where Query act as giver and Key act as receiver. \n",
    "- Value can seem as information extractors, will extract a unique value based on attentions of words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "![image-2.png](./img/attention13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention Mechanisms: Self-Attention\n",
    "\n",
    "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf\n",
    "\n",
    "”They welcomed the **queen** of the united kindom.“ \n",
    "\n",
    "    \n",
    "![image-2.png](./img/attention2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "![image-2.png](./img/transformer_weight_matrix.gif)\n",
    "\n",
    "<[basic_self_attention.ipynb](basic_self_attention.ipynb)>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms: Multi-head Self-Attention\n",
    "\n",
    "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf  \n",
    "\n",
    "They **played** chess\n",
    "\n",
    "动词vs.过去时态被分别表征\n",
    "\n",
    "    \n",
    "![image-2.png](./img/attention0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms: Multi-head Self-Attention\n",
    "\n",
    "https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf  \n",
    "\n",
    "They **played** chess: 动词vs.过去时态被分别表征\n",
    "\n",
    "    \n",
    "![image-2.png](./img/attention3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "![image-2.png](./img/transformer_multi-headed_self-attention-recap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "![image-2.png](./img/transformer_decoding_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "![image-2.png](./img/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms and Transformers\n",
    "\n",
    "http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html\n",
    "    \n",
    "    \n",
    "![image-2.png](./img/attention4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms and Bert\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. \n",
    "\n",
    "![image-2.png](./img/attention5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms and Bert\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "The embeddings of the BERT input sequence are the sum of the token embeddings, segment embeddings, and positional embeddings.\n",
    "\n",
    "![image-2.png](./img/attention6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attention Mechanisms, Transformers, and GPT\n",
    "\n",
    "In 2018, OpenAI published a paper (Improving Language Understanding by Generative Pre-Training) about using natural language understanding using their GPT-1 language model. This model was a proof-of-concept and was not released publicly.\n",
    "\n",
    "![image-2.png](./img/attention7.png)\n",
    "\n",
    "https://www.datacamp.com/blog/what-we-know-gpt4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hugging Face Transformer Course\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/c\n",
    "\n",
    "This course will teach you about natural language processing (NLP) using libraries from the [Hugging Face](https://huggingface.co/) ecosystem — [🤗 Transformers](https://github.com/huggingface/transformers), [🤗 Datasets](https://github.com/huggingface/datasets), [🤗 Tokenizers](https://github.com/huggingface/tokenizers), and [🤗 Accelerate](https://github.com/huggingface/accelerate) — as well as the [Hugging Face Hub](https://huggingface.co/models). It’s completely free and without ads.\n",
    "\n",
    "\n",
    "![image-2.png](./img/attention8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hands-on Machine Learning \n",
    "\n",
    "\n",
    "![image-2.png](./img/attention9.png)\n",
    "\n",
    "All the code examples in this book https://github.com/ageron/handson-ml3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dive into Deep Learning \n",
    "\n",
    "![image-2.png](./img/attention10.png)\n",
    "\n",
    "All the code examples in this book http://d2l.ai/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T12:15:19.223210Z",
     "start_time": "2024-07-24T12:15:19.219011Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning for Economists\n",
    "\n",
    "EconDL is a comprehensive resource detailing applications of **Deep Learning for Economists**: https://econdl.github.io . This website contains user-friendly software and dataset resources, and a knowledge base. https://arxiv.org/pdf/2407.15339\n",
    "\n",
    "MELISSA DELL, Professor of Economics at Harvard University. https://dell-research-harvard.github.io/\n",
    "\n",
    "![image-2.png](./img/Dell_Melissa_hi_res.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/chengjun2.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
